# 拜占庭将军问题

拜占庭有10个将军要出去打仗，困扰这些将军的问题是，他们不确定他们中是否有叛徒，叛徒可能擅自变更进攻决定。在这种状态下，拜占庭将军们能否找到一种分布式的协议来让他们能够远程协商，从而就进攻问题达成一致？这就是著名的拜占庭将军问题。

**背景：**拜占庭派10支军队去围攻一个强大的敌人，至少6支军队同时进攻才可以取胜，否则不进攻。

**难题：**其中一些将军是叛徒，会发布假消息或者相反的进攻意图（对应分布式系统中某些节点无法通信或者宕机）。

**目的：**所有将军远程协商最终达成一致。

## 拜占庭容错

在拜占庭将军问题中，有两种错误模型

* 故障行为：指网络发生故障，信息不可达
* 恶意行为：指通信的过程中有人恶意篡改结果，信息不可信

拜占庭容错是指能够解决恶意行为，而非拜占庭容错，又叫故障容错，解决的是分布式系统中存在故障，但不存在恶意节点的共识问题，比如进程崩溃，服务器硬件故障

一般而言，在可信环境（比如企业内网）中，系统具有故障容错的能力就可以了，常见的算法有二阶段提交，TCC，Paxos算法，ZAB协议，Raft算法，Gossip协议，Quorum NWR算法

而在不可信的环境中（信息不可信），这时系统需要具备拜占庭容错能力，常见的拜占庭容错算法有POW算法，PBFT算法

## 兰伯特论文中提到的拜占庭容错的解决方案

兰伯特论文中提出了两种拜占庭容错的解决方案

### 口头协议（口信消息）

兰伯特通过推导得出了这么一个结果，集群中的节点个数为n，恶意节点的个数为m，如果`n > 3m`，则可以通过算法保证集群中忠诚的节点达到共识（恶意节点的结果不用考虑）

具体的算法方案如下

* 在现有的节点中选出一个指挥官Commander，其他的节点被称为副将Assistant

* Commander把自己的命令发送给每个Assistant

* 每个Assistant在接受到Commander命令的时候，因为不知道Commander是不是叛徒，所以需要向其他Assistant询问Commander发送给他的命令

  > 假如Commander是叛徒，对一部分Assistant发送进攻的命令，对另一部分Assistant发送撤退的命令，因为`n > 3m`，所以每个Assistant可以通过询问其他的Assistant收到的Commander的命令之后，进行一个多数派读，最后得出一个共识的结果（这个结果并不一定是正确的，因为当前正确的结果可能是进攻，然而因为Commander是叛徒，所以对大多数Assistant发出的是撤退命令，然后Assistant通过多数派的读获取到的共识结果是撤退）

* 每个AssistantA向AssistantB询问Commander的命令的时候，因为AssistantA并不能确定AssistantB是不是叛徒，所以AssistantA还需要向集群中其他Assistant询问AssistantB告诉他们AssistantB获取到的Commander的结果是什么，可以看出这是一个**递归**的过程，相当于在除了Commander的其他节点所组成的集群中，AssistantB充当了新的Commander。

* 既然这个算法是一个递归的算法，就必然逃不过递归的三要素，分解问题，合并结果，分解问题的终结条件（跳出递归的条件）

  * 分解问题：在当前集群中任意选出一个Commander，其余的为Assistant，Assistant要对Commander命令达到共识，而Assistant之间也需要对别的Assistant转述Commander的命令达成共识，这样每次需要达成共识的节点的数量就会减1
  * 合并问题：每个Assistant对集群中其他Assistant对Commander命令的转述进行统计，进行一个多数派的读，得出结果
  * 终结条件：这个兰伯特直接在论文中给出了结果，如果叛徒数量为m，那么需要进行m+1次询问，也就是m次递归

上述只是基于兰伯特给出的算法过程得出的一个结论，至于为什么需要`n > 3m`，需要阅读兰伯特的论文理解其推导过程

**这种方案都只考虑到了消息的不可信的情况，理想的认为各个节点发送的消息都是可达的并且没有延迟，没有到因为考虑网络延迟或者节点宕机导致的消息不可达或者消息延迟的情况**，所以只能做理论的证明，不能用在实际工程中

### 书面协议（签名消息）

书面协议比起口头协议要简单一点，大概的意思就是利用非对称加密来避免Assistant转述Commander命令的时候篡改命令的内容，如果发现内容被篡改就认定这个Assistant为恶意节点

# CAP理论

CAP 理论是对分布式系统的特性做了一个高度的抽象，变成了三大指标：

- 一致性（Consistency）
- 可用性（Availability）：接口阻塞或者服务下线，导致服务不可用
- 分区容错性（Partition Tolerance）：信息不可达

## 一致性

一般来讲，我们将一致性分为三类。

* 强一致性：保证写操作完成后，任何后续访问都能读到更新后的值。
* 弱一致性：写操作完成后，系统不能保证后续的访问都能读到更新后的值。
* 最终一致性：保证如果对某个对象没有新的写操作了，最终所有后续访问都能读到相同的最近更新的值。

### 强一致性

强一致性是具有多种含义的

* 系统状态一致性：内部状态的一致，可以理解为操作要么全部成功，要么全部失败，各节点处于一个一致的状态，这个其实就是指的是单机事务的原子性
* 数据访问一致性：对外一致，CAP理论中的一致性，可以理解为写操作完成后，能否读取到最新数据，如果写操作完成后，读操作都能读取到最新数据，那么就是强一致性

可以看出系统状态一致性其实是兼容数据访问一致性的，只要内部状态一致就一定能满足数据访问的一致性，两者是从两个角度看待数据的一致性

## 一致性和共识的区别

* 共识：各节点就指定值（Value）达成共识，而且达成共识后的值，就不再改变了。
* 一致性：是指写操作完成后，能否从各节点上读到最新写入的数据，如果立即能读到，就是强一致性，如果最终能读到，就是最终一致性。

所以其实 Paxos 和 Raft 包括 ZAB 是共识算法

## 理解CAP

理解 CAP 理论的最简单方式是假设两个节点分别处于两个不同的网络中，并且这两个网络是不连通的（符合了P，这是大前提），允许至少一个节点更新状态会导致数据不一致，即丧失了 C 性质。如果为了保证数据一致性，将分区一侧的节点设置为不可用，那么又丧失了 A 性质。除非两个节点可以互相通信，才能既保证 C 又保证 A，这又会导致丧失 P 性质。一般来说跨区域的系统，设计师无法舍弃 P 性质，那么就只能在数据一致性和可用性上做一个艰难选择。

## C和A之间做选择

对于一个分布式系统而言，我们要始终假设网络是不可靠的，因此分区容错性是对一个分布式系统最基本的要求，我们的切入点更多的是尝试在可用性和一致性之间寻找一个平衡点，但这也并非要求我们在系统设计时一直建立在网络出现分区的场景之上，然后对一致性和可用性在选择时非此即彼。

实际上 Eric Brewer 在 2012 年就曾指出 **CAP 理论证明不能同时满足一致性、可用性，以及分区容错性的观点在实际系统设计指导上存在一定的误导性**。传统对于 CAP 理论的理解认为在设计分布式系统时必须满足 P，然后在 C 和 A 之间进行取舍，这是片面的，实际中网络出现分区的可能性还是比较小的，尤其是目前网络环境正在变得越来越好，甚至许多系统都拥有专线支持，**所以在网络未出现分区时，还是应该兼顾 A 和 C**；

另外就是对于一致性、可用性，以及分区容错性三者在度量上也应该有一个评定范围，最简单的以可用性来说，当有多少占比请求出现响应超时才可以被认为是不满足可用性，而不是一出现超时就认为是不可用的；最后我们需要考虑的一点就是分布式系统一般都是一个比较大且复杂的系统，我们应该从更小的粒度上对各个子系统进行评估和设计，而不是简单的从整体上认为需要满足 P，而在 A 和 C 之间做取舍，一些子系统可能需要尽可能同时满足三者。

让分布式集群始终对外提供可用的一致性服务一直是富有挑战和趣味的一项任务。暂且抛开可用性，拿一致性来说，对于关系型数据库我们通常利用事务来保证数据的强一致性，当我们的数据量越来越大，大到单库已经无法承担时，我们不得不采取分库分表的策略对数据库实现水平拆分，构建分布式数据库集群，这样可以将一个数据库的压力分摊到多个数据库，极大的提升了数据库的存储和响应能力，但是拆分之后也为我们使用数据库带来了许多的限制，比如主键的全局唯一、联表查询、数据聚合等等，另外一个相当棘手的问题就是数据库的事务由原先的单库事务变成了现在的分布式事务。

分布式事务的实现并不是很难，比如下文要展开的两阶段提交（2PC：Two-Phrase Commit）和三阶段提交（3PC：Three-Phrase Commit）都给我们提供了思路，但是如果要保证数据的强一致性，并要求对外提供可用的服务，就变成了一个几乎不可能的任务（至少目前是），因此很多分布式系统对于数据强一致性都敬而远之。

## BASE理论

BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的简写

BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的结论，是基于CAP定理逐步演化而来的，**其核心思想是即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）**

* 强一致性：指系统在时时刻刻都保持一致性

## 基本可用

基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性，但请注意，这绝不等价于系统不可用，以下两个就是“基本可用”的典型例子。

- 响应时间上的损失：正常情况下，一个在线搜索引擎需要0.5秒内返回给用户相应的查询结果，但由于出现异常（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1~2秒。
- 功能上的损失：正常情况下，在一个电子商务网站上进行购物，消费者几乎能够顺利地完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。

## 弱状态

弱状态也称为软状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据听不的过程存在延时。

## 最终一致性

最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

# 分布式协议和算法

|            | 拜占庭容错 |          一致性          | 性能 | 可用性 |
| :--------: | :--------: | :----------------------: | :--: | :----: |
|    2PC     |     否     |  强一致性（内部一致性）  |  低  |   低   |
|    TCC     |     否     | 最终一致性（内部一致性） |  低  |   低   |
|   Paxos    |     否     | 最终一致性（外部一致性） | 中低 |   中   |
|    ZAB     |     否     |  强一致性（外部一致性）  |  中  |   中   |
|    Raft    |     否     |  强一致性（外部一致性）  |  中  |   中   |
|   Gossip   |     否     | 最终一致性（外部一致性） |  高  |   高   |
| Quorum NWR |     否     |  强一致性（外部一致性）  |  中  |   中   |
|    PBFT    |     是     |           N/A            |  低  |   中   |
|    POW     |     是     |           N/A            |  低  |   中   |

## 两阶段提交和三阶段提交

### 两阶段提交（2PC：Two-Phrase Commit）

* 协调者（单点）：负责协调各个本地资源的提交和回滚；
* 参与者：负责本地事务的执行

##### **第一阶段：投票阶段**

该阶段的主要目的在于打探数据库集群中的各个参与者是否能够正常的执行事务，具体步骤如下：

1. 协调者向所有的参与者发送事务执行请求，并等待参与者反馈事务执行结果。
2. 事务参与者收到请求之后，执行事务，但不提交，并记录事务日志。
3. 参与者将自己事务执行情况反馈给协调者，同时阻塞等待协调者的后续指令。

##### **第二阶段：事务提交阶段**

在第一阶段协调者的询盘之后，各个参与者会回复自己事务的执行情况，这时候存在三种可能：

1. 所有的参与者回复能够正常执行事务
2. 一个或多个参与者回复事务执行失败
3. 协调者等待超时

对于第一种情况，协调者将向所有的参与者发出提交事务的通知，具体步骤如下：

1. 协调者向各个参与者发送commit通知，请求提交事务。
2. 参与者收到事务提交通知之后，执行commit操作，然后释放占有的资源。
3. 参与者向协调者返回事务commit结果信息。

![事务提交时序图](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929121349rb46Ts.png)

对于第二、三种情况，协调者均认为参与者无法正常成功执行事务，为了整个集群数据的一致性，所以要向各个参与者发送事务回滚通知，具体步骤如下：

1. 协调者向各个参与者发送事务rollback通知，请求回滚事务。
2. 参与者收到事务回滚通知之后，执行rollback操作，然后释放占有的资源。
3. 参与者向协调者返回事务rollback结果信息。

![事务回滚时序图](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929121420Q4DNdq.png)

两阶段提交协议解决的是分布式系统中数据强一致性问题，其原理简单，易于实现，但是缺点也是显而易见的，主要缺点如下：

- **单点问题**

协调者在整个两阶段提交过程中扮演着举足轻重的作用，一旦协调者所在服务器宕机，那么就会影响整个数据库集群的正常运行，比如在第二阶段中，如果协调者因为故障不能正常发送事务提交或回滚通知，那么参与者们将一直处于阻塞状态，整个数据库集群将无法提供服务。

- **同步阻塞**

两阶段提交执行过程中，所有的参与者都需要听从协调者的统一调度，期间处于阻塞状态而不能从事其他操作，这样效率及其低下。

- **数据不一致性**

两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能，比如在第二阶段中，假设协调者发出了事务commit的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了commit操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。

### 三阶段提交（Three-phase commit）

也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。

与两阶段提交不同的是，三阶段提交有两个改动点。

1. 引入超时机制。同时在协调者和参与者中都引入超时机制。
2. 在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。

也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有`CanCommit`、`PreCommit`、`DoCommit`三个阶段。

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929143932c92JJe.jpg)

#### CanCommit阶段

3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。

> 1. **事务询问** 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。**注意此阶段并没有执行事务，执行事务是在PreCommit阶段，也就是没有加锁**。
> 2. **响应反馈** 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No
> 3. CanCommit阶段的作用主要是对所有的参与者进行一次心跳检测，提前排除宕机或者网络波动导致的服务不可用（当然这一刻的节点的状态不代表下一刻节点的状态，还是会有服务不可用的情况，只是概率降低了）

#### PreCommit阶段

协调者根据参与者在CanCommit阶段的反应情况来决定是否可以记性事务的PreCommit操作。根据响应情况，有以下两种可能。

**假如协调者在CanCommit阶段从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行（这个和两阶段提交的预执行是一样的逻辑）。**

> **1.发送预提交请求** 协调者向参与者发送PreCommit请求，并进入Prepared阶段。
>
> **2.事务预提交** 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中，**对资源进行加锁**。
>
> **3.响应反馈** 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。

**假如在CanCommit阶段有任何一个参与者向协调者发送了No响应，或者等待超时之后(当协调者宕机或网络波动，参与者就会接收超时)，协调者都没有接到参与者的响应，那么就执行事务的中断。**

> **1.发送中断请求** 协调者向所有参与者发送abort请求。
>
> **2.中断事务** 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。

#### doCommit阶段

该阶段进行真正的事务提交，即对所有资源进行释放，也可以分为以下两种情况。

**执行提交**

> **1.发送提交请求** 协调者在PreCommit阶段接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。
>
> **2.事务提交** 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。
>
> **3.响应反馈** 事务提交完之后，向协调者发送Ack响应。
>
> **4.完成事务** 协调者接收到所有参与者的ack响应之后，完成事务。

**中断事务** 协调者在PreCommit阶段没有接收到某个参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。

> **1.发送中断请求** 协调者向所有参与者发送abort请求
>
> **2.事务回滚** 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。
>
> **3.反馈结果** 参与者完成事务回滚之后，向协调者发送ACK消息
>
> **4.中断事务** 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。

在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。 ）

### 2PC与3PC的区别

相对于2PC，3PC主要解决了协调者宕机之后事务阻塞的问题，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。

## PAXOS算法

### 场景化简

假设一个集群包含三个节点 A, B, C，提供只读 key-value 存储服务。只读 key-value 的意思是指，当一个 key 被创建时，它的值就确定下来了，且后面不能修改。

客户端 1 和客户端 2 同时试图创建一个 `X` 键。客户端 1 创建值为 `"leehao.me"` 的 `X` ，客户端 2 创建值为 `"www.leehao.me"` 的 `X`。在这种情况下，集群如何达成共识，实现各节点上 `X` 的值一致呢？

> 重点是要满足集群中所有节点数据的一致性

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/202109292233401Otxa0.png)

实现上面场景最直接能想到的是完全的主从同步方式，而完全的主从同步方式，需要等待集群中所有节点都设置数据之后才返回，延迟比较高，而且集群中任意一个节点不可用就会导致整个集群不可用，容错性低

### 多数派写

多数派写指的是只要一个集群中quorum个节点写入成功，就算写入成功（我们要求的场景是变量一旦被赋值就不会被修改，所以不用考虑后续修改的问题），在读取数据的时候需要多数派读，因为在多数派写的时候写入了quorum个节点，在读取的时候，也读取quorum个节点，这样一定可以读取到变量的值

> quorum个数必须大于集群节点数的一半
>
> 一个集合的任意两个quorum个元素的子集一定有有交集

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929221501e4Rjgl.png)

上面描述的多数派写并不涉及到并发的写，如果是并发的写呢，如下图，同时有两个客户端准备进行多数派写。

![3939ce9a3cd096a884a38bb9daf6d4bb](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929225132CUucEx.png)

* 要解决并发写，最简单的逻辑就是在进行多数派写之前先进行一次多数派读，如果数据已经被设置，那么就这次写就不设置值，如果数据没有被设置，那么这次写就设置值（这点很重要，Paxos算法的核心流程就是这个）

### Paxos 涉及的概念

在 Paxos 算法中，存在提议者（Proposer），接受者（Acceptor），学习者（Learner）三种角色，它们的关系如下：

- 提议者（Proposer）：提议一个值，用于投票表决，可以将上图客户端 1 和客户端 2 看作提议者。实际上，提议者更多是集群内的节点，这里为了演示的方便，将客户端 1 和 2 看作提议者，不影响 Paxos 算法的实质
- 接受者（Acceptor）：对每个提议的值进行投票，并存储接受的值，例如，上图集群内的节点 A、B、C
- 学习者（Learner）：被告知投票的结果，接受达成共识的值，不参与投票的过程，存储接受数据

需要指出的是，一个节点，既可以是提议者，也可以是接受者。

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223409QNKgOa.png)

在 Paxos 算法中，使用**提案**表示一个提议，提案包括提案编号和提议的值。接下来，我们使用 `[n, v]` 表示一个提案，其中，`n` 是提案编号，`v` 是提案的值。

在 Basic Paxos 中，集群中各个节点为了达成共识，需要进行 2 个阶段的协商，即准备（Prepare）阶段和接受（Accept）阶段。

### 准备阶段

假设客户端 1 的提案编号是 1，客户端 2 的提案编号为 5，并假设节点 A, B 先收到来自客户端 1 的准备请求，节点 C 先收到来自客户端 2 的准备请求。
客户端作为提议者，向所有的接受者发送包含提案编号的准备请求。注意在准备阶段，请求中不需要指定提议的值，只需要包含提案编号即可。

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223509APAQbQ.png)

接下来，节点 A，B 接收到客户端 1 的准备请求（提案编号为 1），节点 C 接收到客户端 2 的准备请求（提案编号为 5）。

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223604EUWgBN.png)

集群中各个节点在接收到第一个准备请求的处理：

- 节点 A, B：由于之前没有通过任何提案，所以节点 A，B 将返回“尚无提案”的准备响应，并承诺以后不再响应提案编号小于等于 1 的准备请求，不会通过编号小于 1 的提案
- 节点 C：由于之前没有通过任何提案，所以节点 C 将返回“尚无提案”的准备响应，并承诺以后不再响应提案编号小于等于 5 的准备请求，不会通过编号小于 5 的提案

接下来，当节点 A，B 接收到提案编号为 5 的准备请求，节点 C 接收到提案编号为 1 的准备请求：

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223621iAxfjx.png)

- 节点 A, B：由于提案编号 5 大于之前响应的准备请求的提案编号 1，且节点 A, B 都没有通过任何提案，故均返回“尚无提案”的响应，并承诺以后不再响应提案编号小于等于 5 的准备请求，不会通过编号小于 5 的提案
- 节点 C：由于节点 C 接收到提案编号 1 小于节点 C 之前响应的准备请求的提案编号 5 ，所以丢弃该准备请求，不作响应

### 接受阶段

Basic Paxos 算法第二阶段为接受阶段。当客户端 1，2 在收到大多数节点的准备响应之后会开始发送接受请求。

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223649R6D0gk.png)

- 客户端 1：客户端 1 接收到大多数的接受者（节点 A, B）的准备响应后，根据响应中的提案编号最大的提案的值，设置接受请求的值。由于节点 A, B 均返回“尚无提案”，即提案值为空，故客户端 1 把自己的提议值 `"leehao.me"` 作为提案的值，发送接受请求 `[1, "leehao.me"]`
- 客户端 2：客户端 2 接收到大多数接受者的准备响应后，根据响应中的提案编号最大的提案的值，设置接受请求的值。由于节点 A, B, C 均返回“尚无提案”，即提案值为空，故客户端 2 把自己的提议值 `"www.leehao.me"` 作为提案的值，发送接受请求 `[5, "www.leehao.me"]`

当节点 A, B, C 接收到客户端 1, 2 的接受请求时，对接受请求进行处理：

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/202109292237107F4kQ7.png)

- 节点 A, B, C 接收到接受请求 `[1, "leehao.me"]` ，由于提案编号 1 小于三个节点承诺可以通过的最小提案编号 5，所以提案 `[1, "leehao.me"]` 被拒绝
- 节点 A, B, C 接收到接受请求 `[5, "www.leehao.me"]`，由于提案编号 5 不小于三个节点承诺可以通过的最小提案编号 5 ，所以通过提案 `[5, "www.leehao.me"]`，即三个节点达成共识，接受 `X` 的值为 `"www.leehao.me"`

如果集群中还有学习者，当接受者通过一个提案，就通知学习者，当学习者发现大多数接受者都通过了某个提案，那么学习者也通过该提案，接受提案的值。

### 接受者存在已通过提案的情况

上面例子中，准备阶段和接受阶段均不存在接受者已经通过提案的情况。这里继续使用上面的例子，不过假设节点 A, B 已通过提案 `[5, "www.leehao.me"]`，节点 C 未通过任何提案。
增加一个新的提议者客户端 3，客户端 3 的提案为 `[9，"leehao"]` 。

接下来，客户端 3 执行准备阶段和接受阶段。

客户端 3 向节点 A, B, C 发送提案编号为 9 的准备请求：

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223759BfSj0K.png)

节点 A, B 接收到客户端 3 的准备请求，由于节点 A, B 已通过提案 `[5, "www.leehao.me"]`，故在准备响应中，包含此提案信息。
节点 C 接收到客户端 3 的准备请求，由于节点 C 未通过任何提案，故节点 C 将返回“尚无提案”的准备响应。

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223816iiXmVA.png)

客户端 3 接收到节点 A, B, C 的准备响应后，向节点 A, B, C 发送接受请求。这里需要特点指出，客户端 3 会根据响应中的提案编号最大的提案的值，设置接受请求的值，如果响应中没有值，才使用客户端3自己的值。由于在准备响应中，已包含提案 `[5, "www.leehao.me"]`，故客户端 3 将接受请求的提案编号设置为 9，提案值设置为 `"www.leehao.me"` 即接受请求的提案为 `[9, "www.leehao.me"]`：

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223855gHXjyR.png)

节点 A, B, C 接收到客户端 3 的接受请求，由于提案编号 9 不小于三个节点承诺可以通过的最小提案编号，故均通过提案 `[9, www.leehao.me]`。

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/202109292239131WLDTA.png)

概括来说，Basic Paxos 具有以下特点：

- Basic Paxos 通过二阶段方式来达成共识，即准备阶段和接受阶段
- Basic Paxos 除了达成共识功能，还实现了容错，在少于一半节点出现故障时，集群也能工作
- 提案编号大小代表优先级。对于提案编号，接受者提供三个承诺：
  - 如果准备请求的提案编号，小于等于接受者已经响应的准备请求的提案编号，那么接受者承诺不接受这个准备请求
  - 如果接受请求中的提案编号，小于接受者已经响应的准备请求的提案编号，那么接受者承诺不通过这个提案
  - 如果按受者已通过提案，那些接受者承诺会在准备请求的响应中，包含已经通过的最大编号的提案信息
- 提议者在处理Prepare请求返回的时候，会根据响应中的提案编号最大的提案的值，设置接受请求的值（相当于放弃了自己的值），如果所有返回中的值都为空，就用提议者自己的值

### Paxos解决的问题

* 如何处理首个提案

  > 1. Paxos提案的ID是通过时间戳+serverID生成的，是有序的
  >
  > 2. `如果准备请求的提案编号，小于等于接受者已经响应的准备请求的提案编号，那么接受者承诺不接受这个准备请求`
  >
  > 以上两点保证了在所有提案都还在Prepare阶段的时候无论经过多少次失败和重试，最后终会有一个或多个提案进入Accept阶段，并且最终一定是提案ID最大的提案被接受

* 如何解决冲突提案并达成共识

  > 1. `如果按受者已通过提案，那些接受者承诺会在准备请求的响应中，包含已经通过的最大编号的提案信息`
  > 2. `提议者在处理Prepare请求返回的时候，会根据响应中的提案编号最大的提案的值，设置接受请求的值（相当于放弃了自己的值），如果所有返回中的值都为空，就用提议者自己的值`
  >
  > 这两点说明了Paxos并不关心最终选出的值是哪一个，只需要保证所选出来的值是唯一的，

* 达成共识后如何处理新提案

  > 1. `如果按受者已通过提案，那些接受者承诺会在准备请求的响应中，包含已经通过的最大编号的提案信息`
  > 2. `提议者在处理Prepare请求返回的时候，会根据响应中的提案编号最大的提案的值，设置接受请求的值（相当于放弃了自己的值），如果所有返回中的值都为空，就用提议者自己的值`
  >
  > 当新的提案来到，因为集群中已经达成共识，由于新的提案也需要获取大多数节点支持，**`一个集合的两个包含多数元素的子集一定有交集`**，所以在新的提案进行准备阶段的时候一定会在Prepare响应中收到已经达成共识的值（因为要达成共识，一定是多数节点都accept，这些节点在新提案进行prepare查询的时候会把已经accept的值返回，新的提案会用响应中提案编号最大提案的值来代替自己的值）。所以新的提案会导致提案数变大，但是不会改变已经达成共识的值

* 当系统中已经有提案被Accept，如何处理新的提案

  > 这种情况下要看新的提案能够对哪些Acceptor进行prepare请求，
  >
  > * 如果刚好请求的Acceptor节点已经有接受的提案了，那么新的提案的值会被替换成它所请求的Acceptor中已经接受的最大提案的值
  > * 如果刚好请求的所有Acceptor都没有接受提案，那么新的提案的值可能会被大多数节点接受，变成共识（网络不出毛病的话）

### Paxos的问题

* 无法保证操作的顺序性

  > 当系统中已经有提案被Accept，如何处理新的提案
  >
  > > 这种情况下要看新的提案能够对哪些Acceptor进行prepare请求，
  > >
  > > * 如果刚好请求的Acceptor节点已经有接受的提案了，那么新的提案的值会被替换成它所请求的Acceptor中已经接受的最大提案的值
  > > * 如果刚好请求的所有Acceptor都没有接受提案，那么新的提案的值可能会被大多数节点接受，变成共识（网络不出毛病的话）
  > >
  > > 所以Paxos对于提案的值的设置是有随机性的，可能是新提案的值，也可能是已经被接受的提案的值，无法保证顺序性

## Raft算法

Raft 算法属于 Multi-Paxos 算法，它是在兰伯特 Multi-Paxos 思想的基础上，做了一些简化和限制，比如增加了日志必须是连续的，只支持领导者、跟随者和候选人三种状态，在理解和算法实现上都相对容易许多，除此之外，Raft 算法是现在分布式系统开发首选的共识算法。

[Raft算法动态演示](http://thesecretlivesofdata.com/raft/)

从本质上说，Raft 算法是通过一切以领导者为准的方式，实现一系列值的共识和各节点日志的一致，而这种模型有其局限性

1. 强领导模型对于写功能基本退化单机性能，量大任然会出现性能瓶颈，适得其反。
2. 选举期间会集群将出现短暂不可用现象，影响时长与选举时间相关。
3. 对于读要实现强一致性读只能从Master节点读，从Flower节点读只能实现最终一致性

Raft算法主要解决三个问题，选举领导者（选主），领导者和追随者之间的复制，成员变更问题

### 成员身份

* 领导者：蛮不讲理的霸道总裁，一切以我为准，平常的主要工作内容就是 3 部分，处理写请求、管理日志复制和不断地发送心跳信息，通知其他节点“我是领导者，我还活着，你们现在不要发起新的选举，找个新领导者来替代我。”

* 跟随者：就相当于普通群众，默默地接收和处理来自领导者的消息，当等待领导者心跳信息超时的时候，就主动站出来，推荐自己当候选人。
* 候选人：候选人将向其他节点发送请求投票（RequestVote）RPC 消息，通知其他节点来投票，如果赢得了大多数选票，就晋升当领导者。

需要注意的是，Raft 算法是强领导者模型，集群中只能有一个领导者。

### 领导者选举过程

首先，在初始状态下，集群中所有的节点都是跟随者的状态。

<img src="https://raw.githubusercontent.com/syllr/image/main/uPic/20210930094711xPQKF9.jpg" alt="img" style="zoom:50%;" />

Raft 算法实现了随机超时时间的特性。也就是说，每个节点等待领导者节点心跳信息的超时时间间隔是随机的。

通过上面的图片你可以看到，集群中没有领导者，而节点 A 的等待超时时间最小（150ms），它会最先因为没有等到领导者的心跳信息，发生超时。这个时候，节点 A 就增加自己的任期编号，并推举自己为候选人，先给自己投上一张选票，然后向其他节点发送请求投票 RPC 消息，请它们选举自己为领导者。

<img src="https://static001.geekbang.org/resource/image/aa/9c/aac5704d69f142ead5e92d33f893a69c.jpg" alt="img" style="zoom:50%;" />

如果其他节点接收到候选人 A 的请求投票 RPC 消息，在编号为 1 的这届任期内，也还没有进行过投票，那么它将把选票投给节点 A，并增加自己的任期编号。

<img src="https://static001.geekbang.org/resource/image/a4/95/a4bb6d1fa7c8c48106a4cf040b7b1095.jpg" alt="img" style="zoom:50%;" />

如果候选人在选举超时时间内赢得了大多数的选票，那么它就会成为本届任期内新的领导者。

<img src="https://static001.geekbang.org/resource/image/ff/2c/ffaa3f6e9e87d6cea2a3bfc29647e22c.jpg" alt="img" style="zoom:50%;" />

节点 A 当选领导者后，他将周期性地发送心跳消息，通知其他服务器我是领导者，阻止跟随者发起新的选举，篡权。

<img src="https://static001.geekbang.org/resource/image/0a/91/0a626f52c2e2a147c59c862b148be691.jpg" alt="img" style="zoom:50%;" />

#### 节点间如何通讯？

在 Raft 算法中，服务器节点间的沟通联络采用的是远程过程调用（RPC），在领导者选举中，需要用到这样两类的 RPC：

1. 请求投票（RequestVote）RPC，是由候选人在选举期间发起，通知各节点进行投票；
2. 日志复制（AppendEntries）RPC，是由领导者发起，用来复制日志和提供心跳消息。

#### 什么是任期？

Raft 算法中的领导者也是有任期的，每个任期由单调递增的数字（任期编号）标识，比如节点 A 的任期编号是 1。任期编号是随着选举的举行而变化的，这是在说下面几点。

1. 跟随者在等待领导者心跳信息超时后，推举自己为候选人时，会增加自己的任期号，比如节点 A 的当前任期编号为 0，那么在推举自己为候选人时，会将自己的任期编号增加为 1。
2. 如果一个服务器节点，发现自己的任期编号比其他节点小，那么它会更新自己的编号到较大的编号值。比如节点 B 的任期编号是 0，当收到来自节点 A 的请求投票 RPC 消息时，因为消息中包含了节点 A 的任期编号，且编号为 1，那么节点 B 将把自己的任期编号更新为 1。
3. 在 Raft 算法中约定，如果一个候选人或者领导者，发现自己的任期编号比其他节点小，那么它会立即恢复成跟随者状态。比如分区错误恢复后，任期编号为 3 的领导者节点 B，收到来自新领导者的，包含任期编号为 4 的心跳消息，那么节点 B 将立即恢复成跟随者状态。
4. 还约定如果一个节点接收到一个包含较小的任期编号值的请求，那么它会直接拒绝这个请求。比如节点 C 的任期编号为 4，收到包含任期编号为 3 的请求投票 RPC 消息，那么它将拒绝这个消息。

#### 选举有哪些规则

1. 领导者周期性地向所有跟随者发送心跳消息（即不包含日志项的日志复制 RPC 消息），通知大家我是领导者，阻止跟随者发起新的选举。

2. 如果在指定时间内，跟随者没有接收到来自领导者的消息，那么它就认为当前没有领导者，推举自己为候选人，发起领导者选举。

3. 在一次选举中，赢得大多数选票的候选人，将晋升为领导者。

4. 在一个任期内，领导者一直都会是领导者，直到它自身出现问题（比如宕机），或者因为网络延迟，其他节点发起一轮新的选举。

5. 在一次选举中，**每一个服务器节点最多会对一个任期编号投出一张选票，并且按照“先来先服务”的原则进行投票**。比如节点 C 的任期编号为 3，先收到了 1 个包含任期编号为 4 的投票请求（来自节点 A），然后又收到了 1 个包含任期编号为 4 的投票请求（来自节点 B）。那么节点 C 将会把唯一一张选票投给节点 A，当再收到节点 B 的投票请求 RPC 消息时，对于编号为 4 的任期，已没有选票可投了。

   <img src="https://static001.geekbang.org/resource/image/33/84/3373232d5c10813c7fc87f2fd4a12d84.jpg" alt="img" style="zoom:50%;" />

6. 日志完整性高的跟随者（也就是最后一条日志项对应的任期编号值更大，索引号更大），拒绝投票给日志完整性低的候选人。比如节点 B 的任期编号为 3，节点 C 的任期编号是 4，节点 B 的最后一条日志项对应的任期编号为 3，而节点 C 为 2，那么当节点 C 请求节点 B 投票给自己时，节点 B 将拒绝投票。

   <img src="https://static001.geekbang.org/resource/image/99/6d/9932935b415e37c2ca758ab99b34f66d.jpg" alt="img" style="zoom:67%;" />

选举是跟随者发起的，推举自己为候选人；大多数选票是指集群成员半数以上的选票；大多数选票规则的目标，是为了保证在一个给定的任期内最多只有一个领导者。

其实在选举中，除了选举规则外，我们还需要避免一些会导致选举失败的情况，比如同一任期内，多个候选人同时发起选举，导致选票被瓜分，选举失败。那么在 Raft 算法中，如何避免这个问题呢？答案就是随机超时时间。

1. 跟随者等待领导者心跳信息超时的时间间隔，是随机的；
2. 如果候选人在一个随机时间间隔内，没有赢得过半票数，那么选举无效了，然后候选人发起新一轮的选举，也就是说，等待选举超时的时间间隔，是随机的。

### 如何复制日志

副本数据是以日志的形式存在的，日志是由日志项组成

日志项究竟是什么样子呢？其实，日志项是一种数据格式，它主要包含用户指定的数据，也就是指令（Command），还包含一些附加信息，比如索引值（Log index）、任期编号（Term）。

<img src="https://static001.geekbang.org/resource/image/d5/6d/d5c7b0b95b4289c10c9e0817c71f036d.jpg" alt="img" style="zoom: 67%;" />

* 指令：一条由客户端请求指定的、状态机需要执行的指令。你可以将指令理解成客户端指定的数据。
* 索引值：日志项对应的整数索引值。它其实就是用来标识日志项的，是一个连续的、单调递增的整数号码。
* 任期编号：创建这条日志项的领导者的任期编号。

一届领导者任期，往往有多条日志项。而且**日志项的索引值是连续的**（这一点和ZAB一样，保证了日志的顺序性），Paxos算法不要求日志是连续的（所以Psxos无法保证顺序性），在 Raft 中，日志不仅是数据的载体，日志的完整性还影响领导者选举的结果。也就是说，日志完整性最高的节点才能当选领导者。

#### 复制流程

<img src="https://static001.geekbang.org/resource/image/b8/29/b863dc8546a78c272c965d6e05afde29.jpg" alt="img" style="zoom:67%;" />

1. 接收到客户端请求后，领导者基于客户端请求中的指令，创建一个新日志项，并附加到本地日志中。
2. 领导者通过日志复制 RPC，将新的日志项复制到其他的服务器。
   1. 领导者只是把日志复制给追随者，追随者接收成功就算成功，不会等到追随者把日志执行才返回
   2. 追随者接收日志的时候会判断当前已经复制过的最大日志索引，如果领导者发送过来同步的日志索引没有和最大日志索引连续，则返回失败
   3. 领导者在复制日志给追随者的时候如果追随者返回失败，领导者会发送当前日志的上一条日志，直至成功
   4. 领导者第一次发送成功的日志并不是最新的日志，就会连续发送日志直到最新的日志
3. 当领导者将日志项，成功复制到大多数的服务器上的时候，领导者会将这条日志项应用到它的状态机中（日志就提交了）。
4. 领导者将执行的结果返回给客户端。
5. 当跟随者接收到心跳信息，或者新的日志复制 RPC 消息后，如果跟随者发现领导者已经提交了某条日志项，而它还没应用，那么跟随者就将这条日志项应用到本地的状态机中。

### 成员变更的问题

在集群中进行成员变更的最大风险是，可能会同时出现 2 个领导者。比如在进行成员变更时，节点 A、B 和 C 之间发生了分区错误，节点 A、B 组成旧配置中的“大多数”，也就是变更前的 3 节点集群中的“大多数”，那么这时的领导者（节点 A）依旧是领导者。

> 原来集群中有ABC三个节点，后来增加DE两个节点，在出现网络分区时AB一区，CED一区，因为集群中配置更新的不及时，AB节点会认为集群中只有3个节点（quorum是2），CED会认为集群中有5个节点（quorum是3）
>
> * 如果AB节点发生选举，因为quorum是2，AB节点同时选举A成为leader
> * CED节点发生选举，因为quorum是3，CED同时选举C成为leader
>
> 上面这种情况就会出现有两个领导者的情况

#### 单节点变更解决成员变更问题

单节点变更，就是通过一次变更一个节点实现成员变更。如果需要变更多个节点，那你需要执行多次单节点变更。比如将 3 节点集群扩容为 5 节点集群，这时你需要执行 2 次单节点变更，先将 3 节点集群变更为 4 节点集群，然后再将 4 节点集群变更为 5 节点集群，就像下图的样子。

<img src="https://static001.geekbang.org/resource/image/7e/55/7e2b1caf3c68c7900d6a7f71e7a3a855.jpg" alt="img" style="zoom:67%;" />

在单节点变更的情况下，一次只新增一个节点，**不管旧的集群配置是怎么组成的，旧配置的“大多数”和新配置的“大多数”都会有一个节点是重叠的**，所以无论网络怎么分区，都不会选出两个领导者

> 原来集群中有ABC三个节点，后来增加C这个节点，老集群配置的quorum是2，新集群中节点数量为4，所以quorum是3，要选出两个leader，就必须一个分区里面有2个或以上节点，另一个分区里面有3个或以上节点，而集群中一共只有4个节点，所以不可能选出两个leader

## Gossip协议

Gossip 协议，顾名思义，就像流言蜚语一样，利用一种随机、带有传染性的方式，将信息传播到整个网络中，并在一定时间内，使得系统内的所有节点数据一致。对你来说，掌握这个协议不仅能很好地理解这种最常用的，实现最终一致性的算法，也能在后续工作中得心应手地实现数据的最终一致性。

#### Gossip 的三板斧

Gossip 的三板斧分别是：直接邮寄（Direct Mail）、反熵（Anti-entropy）和谣言传播（Rumor mongering）。

1. 直接邮寄：就是直接发送更新数据，当数据发送失败时，将数据缓存下来，然后重传。从图中你可以看到，节点 A 直接将更新数据发送给了节点 B、D。

<img src="https://static001.geekbang.org/resource/image/40/6e/40890515407ae099b317ebb52342e56e.jpg" alt="img" style="zoom:67%;" />

> 直接邮寄虽然实现起来比较容易，数据同步也很及时，但可能会因为缓存队列满了而丢数据。也就是说，只采用直接邮寄是无法实现最终一致性的

那如何实现最终一致性呢？答案就是反熵。本质上，反熵是一种通过异步修复实现最终一致性的方法。常见的最终一致性系统（比如 Cassandra），都实现了反熵功能。

2. 反熵指的是集群中的节点，每隔段时间就随机选择某个其他节点，然后通过互相交换自己的所有数据来消除两者之间的差异，实现数据的最终一致性，在实现反熵的时候主要有推、拉和推拉三种方式

> 虽然反熵很实用，但是执行反熵时，相关的节点都是已知的，而且节点数量不能太多，如果是一个**动态变化**或节点数比较多的分布式环境（比如在 DevOps 环境中检测节点故障，并动态维护集群节点状态），这时反熵就不适用了。那么当你面临这个情况要怎样实现最终一致性呢？答案就是谣言传播。

3. 谣言传播，广泛地散播谣言，它指的是当一个节点有了新数据后，这个节点变成活跃状态，并周期性地联系其他节点向其发送新数据，直到所有的节点都存储了该新数据

## ZAB协议

ZAB协议是专门为zookeeper实现分布式协调功能而设计。zookeeper主要是根据ZAB协议是实现分布式系统数据一致性，ZAB和Raft一样，都是强领导模型，本质上讲，zab和raft都是通过强领导者模型实现就多值达成共识的

ZAB协议最核心的设计就是**在分布式的情况下保证操作的顺序性**，与之相对应的Paxos协议虽然能保证达成共识后的值不再改变，但它不关心达成共识的值是什么，也无法保证各值（也就是操作）的顺序性，所以zookeeper专门设计了ZAB协议。

### 选举流程

#### 成员身份

* 领导者（Leader）： 作为主（Primary）节点，在同一时间集群只会有一个领导者。需要你注意的是，所有的写请求都必须在领导者节点上执行。
* 跟随者（Follower）：作为备份（Backup）节点， 集群可以有多个跟随者，它们会响应领导者的心跳，并参与领导者选举和提案提交的投票。需要你注意的是，跟随者可以直接处理并响应来自客户端的读请求，但对于写请求，跟随者需要将它转发给领导者处理。
* 观察者（Observer）：作为备份（Backup）节点，类似跟随者，但是没有投票权，也就是说，观察者不参与领导者选举和提案提交的投票。你可以对比着 Paxos 中的学习者来理解。

虽然 ZAB 支持 3 种成员身份，但是它定义了 4 种成员状态。

* LOOKING：选举状态，该状态下的节点认为当前集群中没有领导者，会发起领导者选举。
* FOLLOWING ：跟随者状态，意味着当前节点是跟随者。
* LEADING ：领导者状态，意味着当前节点是领导者。
* OBSERVING： 观察者状态，意味着当前节点是观察者。

为什么多了一种成员状态呢？这是因为 ZAB 支持领导者选举，在选举过程中，涉及了一个过渡状态（也就是选举状态）。

假设投票信息的格式是<proposedLeader, proposedEpoch, proposedLastZxid, node> ，其中：

* proposedLeader，节点提议的，领导者的集群 ID，也就是在集群配置（比如 myid 配置文件）时指定的 ID。
* proposedEpoch，节点提议的，领导者的任期编号。
* proposedLastZxid，节点提议的，领导者的事务标识符最大值（也就是最新提案的事务标识符）。
* node，投票的节点，比如节点 B。

假设一个 ZooKeeper 集群，由节点 A、B、C 组成，其中节点 A 是领导者，节点 B、C 是跟随者（为了方便演示，假设 epoch 分别是 1 和 1，lastZxid 分别是 101 和 102，集群 ID 分别为 2 和 3）。那么如果节点 A 宕机了，会如何选举呢？

<img src="https://raw.githubusercontent.com/syllr/image/main/uPic/20210930144944izkeHf.jpg" alt="img" style="zoom:67%;" />

首先，当跟随者检测到连接领导者节点的读操作等待超时了，跟随者会变更节点状态，将自己的节点状态变更成 LOOKING，然后发起领导者选举

<img src="https://static001.geekbang.org/resource/image/2b/23/2bc48cf03938cc37d3513239da847c23.jpg" alt="img" style="zoom:67%;" />

接着，每个节点会创建一张选票，这张选票是投给自己的，也就是说，节点 B、C 都“自告奋勇”推荐自己为领导者，并创建选票 <2, 1, 101, B> 和 <3, 1, 102, C>，然后各自将选票发送给集群中所有节点，也就是说，B 发送给 B、C，C 也发送给 B、C。

一般而言，节点会先接收到自己发送给自己的选票（因为不需要跨节点通讯，传输更快），也就是说，B 会先收到来自 B 的选票，C 会先收到来自 C 的选票：

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210930151847K0Usnl.jpg)

需要你注意的是，集群的各节点收到选票后，为了选举出数据最完整的节点，对于每一张接收到选票，节点都需要进行领导者 PK，也就将选票提议的领导者和自己提议的领导者进行比较，找出更适合作为领导者的节点，约定的规则如下：

1. 优先检查任期编号（Epoch），任期编号大的节点作为领导者；
2. 如果任期编号相同，比较事务标识符的最大值，值大的节点作为领导者；
3. 如果事务标识符的最大值相同，比较集群 ID，集群 ID 大的节点作为领导者。

如果选票提议的领导者，比自己提议的领导者，更适合作为领导者，那么节点将调整选票内容，推荐选票提议的领导者作为领导者。

当节点 B、C 接收到的选票后，因为选票提议的领导者与自己提议的领导者相同，所以，领导者 PK 的结果，是不需要调整选票信息，那么节点 B、C，正常接收和保存选票就可以了。

<img src="https://static001.geekbang.org/resource/image/4c/96/4c0546cfe2dbc11ed99cab414ec12e96.jpg" alt="img" style="zoom:67%;" />

接着节点 B、C 分别接收到来自对方的选票，比如 B 接收到来自 C 的选票，C 接收到来自 B 的选票：

<img src="https://static001.geekbang.org/resource/image/5d/d5/5d3fe6b30854490faf096826c62df5d5.jpg" alt="img" style="zoom:67%;" />

对于 C 而言，它提议的领导者是 C，而选票（<2, 1, 101, B>）提议的领导者是 B，因为节点 C 的任期编号与节点 B 相同，但节点 C 的事务标识符的最大值比节点 B 的大，那么，按照约定的规则，相比节点 B，节点 C 更适合作为领导者，也就是说，节点 C 不需要调整选票信息，正常接收和保存选票就可以了。

但对于对于节点 B 而言，它提议的领导者是 B，选票（<3, 1, 102, C>）提议的领导者是 C，因为节点 C 的任期编号与节点 B 相同，但节点 C 的事务标识符的最大值比节点 B 的大，那么，按照约定的规则，相比节点 B，节点 C 应该作为领导者，所以，节点 B 除了接收和保存选票信息，还会更新自己的选票为 <3, 1, 102, B>，也就是推荐 C 作为领导者，并将选票重新发送给节点 B、C：

<img src="https://static001.geekbang.org/resource/image/bf/77/bf70a4f1e9c28de2bbd3c134193ae577.jpg" alt="img" style="zoom:67%;" />

接着，当节点 B、C 接收到来自节点 B，新的选票时，因为这张选票（<3, 1, 102, B>）提议的领导者，与他们提议的领导者是一样的，都是节点 C，所以，他们正常接收和存储这张选票，就可以。

<img src="https://static001.geekbang.org/resource/image/04/7c/04ec7fdf75160571c8689f040292d57c.jpg" alt="img" style="zoom:67%;" />

最后，因为此时节点 B、C 提议的领导者（节点 C）赢得大多数选票了（2 张选票），那么，节点 B、C 将根据投票结果，变更节点状态，并退出选举。比如，因为当选的领导者是节点 C，那么节点 B 将变更状态为 FOLLOWING，并退出选举，而节点 C 将变更状态为 LEADING，并退出选举。

<img src="https://static001.geekbang.org/resource/image/a9/03/a9113e0d2653c77ddc41778510784003.jpg" alt="img" style="zoom:67%;" />

**ZAB 本质上是通过“见贤思齐，相互推荐”的方式来选举领导者的。也就说，根据领导者 PK，节点会重新推荐更合适的领导者，最终选举出了大多数节点中数据最完整的节点。**使用元组{选举轮次、事务ID、节点MyID}，加上“见贤思齐”的选举行为，一定可以选出唯一的Leader，不会出现像Raft那样选票被瓜分，选举失败的问题。

> Raft在选举的时候采用的是每次都把票投给第一个过来请求的候选人，这种方式会出现选票被瓜分，没有一个候选人获得半数以上票的情况，所以Raft采用了Random时间重试的机制
>
> ZAB的方式每次一定都会选出一个唯一Leader，但是每次投票发送的信息量也多，实现比较复杂

<img src="https://static001.geekbang.org/resource/image/83/57/83c37bc2a25a4e892cf1a5c3a2c6c457.jpg" alt="img" style="zoom:67%;" />

### 读写操作

#### 场景思考

假如节点 A、B、C 组成的一个分布式集群，我们要设计一个算法，来保证指令（比如 X、Y）执行的顺序性，比如，指令 X 在指令 Y 之前执行，那么我们该如何设计这个算法呢？

<img src="https://static001.geekbang.org/resource/image/55/c9/55dc6f6bf822db027858b8b4fdb89cc9.jpg" alt="img" style="zoom:67%;" />

Raft 可以实现操作的顺序性啊，为什么 ZooKeeper 不用 Raft 呢？

这个问题其实比较简单，因为 Raft 出来的比较晚，直到 2013 年才正式提出，在 2007 年开发 ZooKeeper 的时候，还没有 Raft 呢。

#### ZAB 是如何实现操作的顺序性的？

如果用一句话解释 ZAB 协议到底是什么，我觉得它是：能保证操作顺序性的，基于**主备模式**的原子广播协议。

接下来以 X、Y 指令为例具体演示一下，为什么 ZAB 能实现操作的顺序性，我们现在的需求是要先设置X再设置Y（为了演示方便，我们假设节点 A 为主节点，节点 B、C 为备份节点）。

首先，需要你注意的是，在 ZAB 中，写操作必须在主节点（比如节点 A）上执行。如果客户端访问的节点是备份节点（比如节点 B），它会将写请求转发给主节点。如图所示：

<img src="https://static001.geekbang.org/resource/image/77/6f/770c39b4ea339799bc3ca4a0b0d8266f.jpg" alt="img" style="zoom:67%;" />

接着，当主节点接收到写请求后，它会基于写请求中的指令（也就是 X，Y），来创建一个提案（Proposal），并使用一个唯一的 ID 来标识这个提案。这里我说的唯一的 ID 就是指事务标识符（Transaction ID，也就是 zxid），就像下图的样子。

<img src="https://static001.geekbang.org/resource/image/d0/9b/d0063fa9275ce0a114ace27db326d19b.jpg" alt="img" style="zoom:67%;" />

从图中你可以看到，X、Y 对应的事务标识符分别为 <1, 1> 和 <1, 2>，这两个标识符是什么含义呢？你可以这么理解，事务标识符是 64 位的 long 型变量，有任期编号 epoch 和计数器 counter 两部分组成（为了形象和方便理解，我把 epoch 翻译成任期编号），格式为 ，高 32 位为任期编号，低 32 位为计数器：

* 任期编号，就是创建提案时领导者的任期编号，需要你注意的是，当新领导者当选时，任期编号递增，计数器被设置为零。比如，前领导者的任期编号为 1，那么新领导者对应的任期编号将为 2。
* 计数器，就是具体标识提案的整数，需要你注意的是，每次领导者创建新的提案时，计数器将递增。比如，前一个提案对应的计数器值为 1，那么新的提案对应的计数器值将为 2。

这样做的目的是为了保证事务标识符必须按照顺序、唯一标识一个提案，也就是说，事务标识符必须是唯一的、递增的。

> Raft是通过维护一个全局的计数器，来保证所有事务id都是连续的，其实两者本质都一样都是为了让事务有序

在创建完提案之后，主节点会基于 TCP 协议，并按照顺序将提案广播到其他节点。这样就能保证先发送的消息，会先被收到，保证了消息接收的顺序性。

<img src="https://static001.geekbang.org/resource/image/e5/96/e525af146900c892e0c002affa77d496.jpg" alt="img" style="zoom:67%;" />

你看这张图，X 一定在 Y 之前到达节点 B、C。

然后，当主节点接收到指定提案的“大多数”的确认响应后，该提案将处于提交状态（Committed），主节点会通知备份节点提交该提案。

> 在 Leader 和 Follwer 之间还有一个消息队列，用来解耦他们之间的耦合，解除同步阻塞。
>
> Follwer发生故障，然后重新连接之后，当主节点发送日志给Follwer的时候，Follwer会将发过来的日志的事务ID和自身已经接受过的最大的事务ID做对比，比对结果要么回滚，要么和 Leader 同步。

<img src="https://static001.geekbang.org/resource/image/1d/19/1d3950b6d91845789cce1f0569969419.jpg" alt="img" style="zoom:67%;" />

在这里，需要你注意的是，主节点提交提案是有顺序性的。主节点根据事务标识符大小，按照顺序提交提案，如果前一个提案未提交，此时主节点是不会提交后一个提案的。也就是说，指令 X 一定会在指令 Y 之前提交。

最后，主节点返回执行成功的响应给节点 B，节点 B 再转发给客户端。你看，这样我们就实现了操作的顺序性，保证了指令 X 一定在指令 Y 之前执行。

当写操作执行完后，接下来可能需要执行读操作了。需要注意的是，为了提升读并发能力，Zookeeper 提供的是最终一致性，也就是读操作可以在任何节点上执行，客户端会读到旧数据：

<img src="https://static001.geekbang.org/resource/image/d4/1c/d405381e5fad12730149baa4fae63e1c.jpg" alt="img" style="zoom:67%;" />

如果客户端必须要读到最新数据，怎么办呢？Zookeeper 提供了一个解决办法，那就是 sync 命令。你可以在执行读操作前，先执行 sync 命令，这样客户端就能读到最新数据了。

> 还有一种做法是，让客户端一次读取性读取quorum个节点，然后从所有节点中的数据中选择事务标识符最大版本的数据（多数派读的逻辑）；
>
> 但是Zookeeper并没有提供这种逻辑

### Zookeeper处理写请求

在 ZooKeeper 中，写请求是必须在领导者上处理，如果跟随者接收到了写请求，它需要将写请求转发给领导者，当写请求对应的提案被复制到大多数节点上时，领导者会提交提案，并通知跟随者提交提案。而读请求可以在任何节点上处理，也就是说，ZooKeeper 实现的是最终一致性。

在 ZooKeeper 中，与领导者“失联”的节点，是不能处理读写请求的。比如，如果一个跟随者与领导者的连接发生了读超时，设置了自己的状态为 LOOKING，那么此时它既不能转发写请求给领导者处理，也不能处理读请求，只有当它“找到”领导者后，才能处理读写请求。

当发生分区故障了，C 与 A（领导者）、B 网络不通了，那么 C 将设置自己的状态为 LOOKING，此时在 C 节点上既不能执行读操作，也不能执行写操作

<img src="https://static001.geekbang.org/resource/image/22/ad/22dfaa624590885c4b8406deb445afad.jpg" alt="img" style="zoom:67%;" />

其次，当大多数节点进入到广播阶段的时候，领导者才能提交提案，因为提案提交，需要来自大多数节点的确认。

最后，写请求只能在领导者节点上处理，**所以 ZooKeeper 集群写性能约等于单机**。而读请求是可以在所有的节点上处理的，所以，读性能是能水平扩展的。也就是说，你可以通过分集群的方式来突破写性能的限制，并通过增加更多节点，来扩展集群的读性能。

### ZooKeeper 代码是如何实现读写操作的呢？

#### 如何实现写操作？

<img src="https://static001.geekbang.org/resource/image/c7/8a/c77c241713b154673e15083fd063428a.jpg" alt="img" style="zoom:67%;" />

#### 如何实现读操作？

<img src="https://static001.geekbang.org/resource/image/f4/6d/f405d2a81f374e6e63b49c469506f26d.jpg" alt="img" style="zoom:67%;" />

### 集群之间同步数据

上面我们讨论了ZooKeeper集群如何选出主节点和主节点如何保证写入的顺序性，在选出主节点后主节点又是怎么把日志同步给各个follwer节点呢？

1. 当集群中选出Leader之后，所有的Follower做的第一件事就是使用FOLLOWERINFO命令把自己的epoch和myid信息发送给Leader

2. 当收到半数以上的Follwer信息之后，会从这些Follwer发送的epoch中选出最大的，然后+1作为自己的新的epoch，然后通过LEAERINFO命令发送给所有Follower

3. Follower接收到新的epoch之后将新的epoch记录下来，然后使用ACKEPOCH回复Leader并带上自己这边最大的zxid（事务id），表示刚刚的LEADERINFO收到了

4. Leader收到Follwer发送的ACKEPOCH之后，会根据各个Follower的信息给出不同的同步策略：

   1. DIFF，如果 Follower 的记录和 Leader 的记录相差的不多，使用增量同步的方式将一个一个写请求发送给 Follower
   2. TRUNC，这个情况的出现代表 Follower 的 zxid 是领先于当前的 Leader 的（可能是以前的 Leader），需要 Follower 自行把多余的部分给截断，降级到和 Leader 一致
   3. SNAP，如果 Follower 的记录和当前 Leader 相差太多，Leader 直接将自己的整个内存数据发送给 Follower

   > 当follwer宕机，重新上线之后也会进行上面的步骤1，3，4，来进行同步

## Raft对比ZAB

Raft 算法和 ZAB 协议很类似，比如主备模式（也就是领导者、跟随者模型）、日志必须是连续的、以领导者的日志为准来实现日志一致等等。

* 领导者选举：ZAB 采用的“见贤思齐、相互推荐”的快速领导者选举（Fast Leader Election），Raft 采用的是“一张选票、先到先得”的自定义算法。在我看来，Raft 的领导者选举，需要通讯的消息数更少，选举也更快。

* 日志复制：Raft 和 ZAB 相同，都是以领导者的日志为准来实现日志一致，而且日志必须是连续的，也必须按照顺序提交。

* 读操作和一致性：ZAB 的设计目标是操作的顺序性，在 ZooKeeper 中默认实现的是最终一致性，读操作可以在任何节点上执行；而 Raft 的设计目标是强一致性（也就是线性一致性），所以 Raft 更灵活，Raft 系统既可以提供强一致性，也可以提供最终一致性。

* 写操作：Raft 和 ZAB 相同，写操作都必须在领导者节点上处理。

  > Zookeeper通过Leader来主导写操作，保证了顺序一致性。当一半以上的节点返回已写入，就返回客户端已写入，但是这时候只是部分节点写入，有的节点可能还没有同步上数据，所以读取备份节点可能不是最新的。同时Zookeeper的**单一视图特征**，保证客户端看到的数据不会比在之前服务器上所看到的更老。

* 成员变更：Raft 和 ZAB 都支持成员变更，其中 ZAB 以动态配置（dynamic configuration）的方式实现的。那么当你在节点变更时，不需要重启机器，集群是一直运行的，服务也不会中断。

* 其他：相比 ZAB，Raft 的设计更为简洁，比如 Raft 没有引入类似 ZAB 的成员发现和数据同步阶段，而是当节点发起选举时，递增任期编号，在选举结束后，广播心跳，直接建立领导者关系，然后向各节点同步日志，来实现数据副本的一致性。在我看来，ZAB 的成员发现，可以和领导者选举合到一起，类似 Raft，在领导者选举结束后，直接建立领导者关系，而不是再引入一个新的阶段；数据同步阶段，是一个冗余的设计，可以去除的，因为 ZAB 不是必须要先实现数据副本的一致性，才可以处理写请求，而且这个设计是没有额外的意义和价值的。

# 分布式事务

分布式事务有4种解决方案

* 2PC

* TCC：TCC事务处理流程和 2PC 二阶段提交类似，不过 2PC通常都是在跨库的DB层面，而TCC本质就是一个应用层面的2PC

  ![在这里插入图片描述](https://raw.githubusercontent.com/syllr/image/main/uPic/20211009224251aYURSa.png)

  假设用户下单操作来自3个系统下单系统、资金账户系统、红包账户系统，下单成功需要同时调用资金账户服务和红包服务完成支付

  假设购买商品1000元，使用账户红包200元，余额800元，确认支付。

  - **Try操作**
    tryX 下单系统创建待支付订单
    tryY 冻结账户红包200元
    tryZ 冻结资金账户800元
  - **Confirm操作**
    confirmX 订单更新为支付成功
    confirmY 扣减账户红包200元
    confirmZ 扣减资金账户800元
  - **Cancel操作**
    cancelX 订单处理异常，资金红包退回，订单支付失败
    cancelY 冻结红包失败，账户余额退回，订单支付失败
    cancelZ 冻结余额失败，账户红包退回，订单支付失败

* 本地消息表：本地消息表与业务数据表处于同一个数据库中，这样就能利用本地事务来保证在对这两个表的操作满足事务特性，并且使用了消息队列来保证最终一致性。

  1. 在分布式事务操作的一方完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中。
  2. 之后将本地消息表中的消息转发到 Kafka 等消息队列中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发。
  3. 在分布式事务操作的另一方从消息队列中读取一个消息，并执行消息中的操作。

  ![img](https://raw.githubusercontent.com/syllr/image/main/uPic/2021100922442271oakQ.jpg)

* RocketMQ事务消息
