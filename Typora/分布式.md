# 拜占庭将军问题

拜占庭有10个将军要出去打仗，困扰这些将军的问题是，他们不确定他们中是否有叛徒，叛徒可能擅自变更进攻决定。在这种状态下，拜占庭将军们能否找到一种分布式的协议来让他们能够远程协商，从而就进攻问题达成一致？这就是著名的拜占庭将军问题。

**背景：**拜占庭派10支军队去围攻一个强大的敌人，至少6支军队同时进攻才可以取胜，否则不进攻。

**难题：**其中一些将军是叛徒，会发布假消息或者相反的进攻意图（对应分布式系统中某些节点无法通信或者宕机）。

**目的：**所有将军远程协商最终达成一致。

## 拜占庭容错

在拜占庭将军问题中，有两种错误模型

* 故障行为：指网络发生故障，信息不可达
* 恶意行为：指通信的过程中有人恶意篡改结果，信息不可信

拜占庭容错是指能够解决恶意行为，而非拜占庭容错，又叫故障容错，解决的是分布式系统中存在故障，但不存在恶意节点的共识问题，比如进程崩溃，服务器硬件故障

一般而言，在可信环境（比如企业内网）中，系统具有故障容错的能力就可以了，常见的算法有二阶段提交，TCC，Paxos算法，ZAB协议，Raft算法，Gossip协议，Quorum NWR算法

而在不可信的环境中（信息不可信），这时系统需要具备拜占庭容错能力，常见的拜占庭容错算法有POW算法，PBFT算法

## 兰伯特论文中提到的拜占庭容错的解决方案

兰伯特论文中提出了两种拜占庭容错的解决方案

### 口头协议（口信消息）

兰伯特通过推导得出了这么一个结果，集群中的节点个数为n，恶意节点的个数为m，如果`n > 3m`，则可以通过算法保证集群中忠诚的节点达到共识（恶意节点的结果不用考虑）

具体的算法方案如下

* 在现有的节点中选出一个指挥官Commander，其他的节点被称为副将Assistant

* Commander把自己的命令发送给每个Assistant

* 每个Assistant在接受到Commander命令的时候，因为不知道Commander是不是叛徒，所以需要向其他Assistant询问Commander发送给他的命令

  > 假如Commander是叛徒，对一部分Assistant发送进攻的命令，对另一部分Assistant发送撤退的命令，因为`n > 3m`，所以每个Assistant可以通过询问其他的Assistant收到的Commander的命令，这样就能得到Commander对所有Assistant的命令（每个Assistant都能得到一个相同的命令列表），最后得出一个共识的结果

* 每个AssistantA向AssistantB询问Commander的命令的时候，因为AssistantA并不能确定AssistantB是不是叛徒，所以AssistantA还需要向集群中其他Assistant询问AssistantB告诉他们AssistantB获取到的Commander的结果是什么，可以看出这是一个**递归**的过程，相当于在除了Commander的其他节点所组成的集群中，AssistantB充当了新的Commander。

* 既然这个算法是一个递归的算法，就必然逃不过递归的三要素，分解问题，合并结果，分解问题的终结条件（跳出递归的条件）

  * 分解问题：在当前集群中任意选出一个Commander，其余的为Assistant，Assistant要对Commander命令达到共识，而Assistant之间也需要对别的Assistant转述Commander的命令达成共识，这样每次需要达成共识的节点的数量就会减1
  * 合并问题：每个Assistant对集群中其他Assistant对Commander命令的转述进行统计，每个Assistant获取到的Commander的命令集合都是一样的，同样的输入，同样的处理方式，得出同样结果，这样每个Assistant之间就能达成共识
  * 终结条件：这个兰伯特直接在论文中给出了结果，如果叛徒数量为m，那么需要进行m+1次询问，也就是m次递归

上述只是基于兰伯特给出的算法过程得出的一个结论，至于为什么需要`n > 3m`，需要阅读兰伯特的论文理解其推导过程

### 书面协议（签名消息）

书面协议比起口头协议要简单一点，大概的意思就是利用非对称加密算法的签名功能来避免Assistant转述Commander命令的时候篡改命令的内容，如果发现内容被篡改就认定这个Assistant为恶意节点

* 当Commander是叛徒，Assistant需要向别的Assistant询问Commander发出的命令

  > 因为不知道别的Assistant是不是叛徒，如果其中一个Assistant是叛徒，则这个Assistant可以和Commander勾结，获取Commander的密钥修改Commander的数据，所以需要多次询问，这一点和口头协议差别不大

* 当Commander不是叛徒，Assistant需要向Assistant询问Commander发出的命令，但是因为签名机制Assistant不能修改Commander的命令，所以不需要再去询问别的Assistant

签名消息的拜占庭问题之解，也是需要进行 m+1 轮（其中 m 为叛将数）。你也可以从另外一个角度理解：n 位将军，能容忍 (n - 2) 位叛将（只有一位忠将没有意义，因为此时不需要达成共识了）。关于这个公式，记住就好了，推导过程可以参考论文。

## 总结

无论是口头协议还是书面协议，都是通过Assistant之间的多次交换数据使得每一个Assistant都能知道Commander对所有Assistant发出的命令

> 比如Commander对AssistantA发出的命令为X=1，对AssistantB发出的命令为X=2，对AssistantC发出的命令为X=3，最后无论是AssistantA，还是AssistantB，AssistantC都能获取到下面的数据
>
> | AssistantA获取到Commander的命令 | AssistantB获取到Commander的命令 | AssistantC获取到Commander的命令 |
> | :-----------------------------: | :-----------------------------: | :-----------------------------: |
> |               X=1               |               X=2               |               X=3               |
>
> 所以所有Assistant节点可以基于同一份数据得出共识的答案

# CAP理论

CAP 理论是对分布式系统的特性做了一个高度的抽象，变成了三大指标：

- 一致性（Consistency）
- 可用性（Availability）：接口阻塞，接口返回失败或者服务下线，导致服务不可用
- 分区容错性（Partition Tolerance）：信息不可达

## 一致性

一般来讲，我们将一致性分为三类。

* 强一致性：保证写操作完成后，任何后续访问都能读到更新后的值。
* 弱一致性：写操作完成后，系统不能保证后续的访问都能读到更新后的值。
* 最终一致性：保证如果对某个对象没有新的写操作了，最终所有后续访问都能读到相同的最近更新的值。

### 强一致性

强一致性是具有多种含义的

* 系统状态一致性：内部状态的一致，可以理解为操作要么全部成功，要么全部失败，各节点处于一个一致的状态，这个其实就是指的是单机事务的原子性
* 数据访问一致性：对外一致，CAP理论中的一致性，可以理解为写操作完成后，能否读取到最新数据，如果写操作完成后，读操作都能读取到最新数据，那么就是强一致性（除非读取失败。失败就意味着破坏可用性）；

可以看出系统状态一致性其实是兼容数据访问一致性的，只要内部状态一致就一定能满足数据访问的一致性，两者是从两个角度看待数据的一致性

## 一致性和共识的区别

* 共识：各节点就指定值（Value）达成共识，而且达成共识后的值，就不再改变了。
* 一致性：是指写操作完成后，能否从各节点上读到最新写入的数据，如果立即能读到，就是强一致性，如果最终能读到，就是最终一致性。一致性强调的数据状态的前后一致。

Paxos 和 Raft 包括 ZAB 是共识算法

## C和A之间做选择

大部分人对CAP理论有个误解，认为无论在什么情况下，分布式系统都只能在C和A中选择一个，其实，在不存在网络分区的情况下，也就是分布式系统正常运行时，（这也是系统在绝大部分时候所处的状态）

* 也就是说在不需要P的时候，C和A可以同时保证
* 只有当发生分区故障时，也就是说需要P时， 才会在A和C中做出选择

C 和 A 的选择可以看作是处理异常的手段，网络工作正常的时候 CA 都是要保证的！而且如果读操作会读到旧数据，影响到了系统运行或业务运行（也就是说会有负面的影响），推荐选择 C，否则选 A。

# BASE理论

BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的简写

BASE 理论是 CAP 理论中的 AP 的延伸，而它的核心就是基本可用（Basically Available）和最终一致性（Eventually consistent）

## 基本可用

基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性，但请注意，这绝不等价于系统不可用，一般来说可以考虑以下4种方式应实现基本可用

- 流量削峰
- 延迟响应（每个请求排队等待，将每个请求的响应时间拉长，但是不影响吞吐量）
- 体验降级
- 过载保护（当请求数量大于系统最大处理能力时，直接将请求丢弃，这种方式是业务有损的，不到万不得已不要用这种方式）

## 弱状态

弱状态也称为软状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。

## 最终一致性

系统中所有的数据副本在经过一段时间的同步后，最终能够达到一个一致的状态。也就是说，在数据一致性上，存在一个短暂的延迟

### 实现最终一致性

实现最终一致性有两个步骤

* 消息传播：当一个节点接收到请求之后把这个节点的数据复制到别的节点中

* 修复一致性：
  * 读时修复：在读取数据时，检测数据的不一致，进行修复，在读取数据时，如果检测到不同节点的副本数据不一致，系统就自动修复数据

    > 在读取集群数据的时候如果是多数派读，比如读取ZK集群每次读取quorum个节点的数据，通过数据的版本号得到最新的数据，如果发现其他节点数据的版本号是老版本号就将这些节点的数据修复

  * 写时修复：在写入数据，检测到数据的不一致时，进行修复

    > raft的主从复制其实就是写时复制，在领导者接收到写请求的时候，将自己的日志复制给跟随者，如果发现跟随者的日志没有提交或者日志不全的，会修复

  * 异步修复：这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复（反墒）

    > 反墒其实是一个抽象的概念，墒在信息学里面指的就是混乱程度，反墒就是减少系统中的混乱程度也就是减少不同节点中数据的差异性

# 分布式事务

## 1PC（分段式事务）

### 消息解耦

假如现在有ServiceA和ServiceB，我们需要确保ServiceA的事务和ServiceB的事务一起执行

我们通常会将两个系统进行解耦，不直接使用服务调用的方式进行交互。其业务实现步骤通常为：

1. ServiceA创建订单并入库。
2. ServiceA发送消息到MQ。
3. ServiceB接收到消息，执行事务
4. ServiceA提交事务
5. ServiceB提交事务

![利用MQ解耦不同系统的事务](https://raw.githubusercontent.com/syllr/image/main/uPic/20211024232800NcS32U.svg)

### 方案弊端

* 如果消息发送成功，在提交事务的时候JVM突然挂掉，事务没有成功提交，导致两个系统之间数据不一致。
* 由于消息是在事务提交之前提交，发送的消息内容是订单实体的内容，会造成在消费端进行消费时如果需要去验证订单是否存在时可能出现订单不存在。

主要原因是发送MQ和ServiceA的本地事务并不是原子性的

### 方案优化

#### 本地消息表

利用ServiceA本地事务的原子性，将ServiceA的事务和发送消息原子化：本地消息表与业务数据表处于同一个数据库中，这样就能利用本地事务来保证在对这两个表的操作满足事务特性，并且使用了消息队列来保证最终一致性。

1. 在分布式事务操作的一方完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中。
2. 之后将本地消息表中的消息利用Job转发到 Kafka 等消息队列中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发。
3. 在分布式事务操作的另一方从消息队列中读取一个消息，并执行消息中的操作。

![本地消息表方法处理分布式事务](https://raw.githubusercontent.com/syllr/image/main/uPic/202110242328147Gb8YO.svg)

#### [RocketMQ事务消息](https://github.com/syllr/leanring_notes/blob/master/Typora/RocketMQ.md#%E6%B6%88%E6%81%AF%E4%BA%8B%E5%8A%A1)

### 总结

ServiceA和ServiceB两个服务的事务之间是相互独立的，所以需要一个消息系统同步各个服务的事务状态，不过分段式方案中的消息系统只有通知的功能，只能做到ServiceA的事务操作和消息的通知是原子化的

* ServiceA事务执行成功，能保证ServiceB的事务一定执行，但是不能保证ServiceB的事务一定执行成功，所以这时需要进行
  * 业务兜底，保证ServiceB事务的执行成功
  * 事务补偿，当ServiceB的事务执行失败的时候，对ServiceA的事务进行处理，或者走人工线下处理

可以看出分段式事务处理的逻辑很简单，利用消息系统来解耦，性能也非常高，但是只实现了事务的一部分原子性

## 2PC/3PC

### 两阶段提交（2PC：Two-Phrase Commit）

2PC协议将分段式事务中的消息系统升级成为事务管理器，利用事务管理器管理各个服务中的事务，将分布式系统中的各个节点抽象为两种角色

* 事务管理器（协调者）（transaction manager）：分布式事务的核心管理者，事务管理器与每个资源管理器（resource  manager）进行通信，协调并完成事务的处理，事务的各个分支由唯一命名进行标识。
* 资源管理器（参与者）（resource manager）：用来管理系统资源，是通向事务资源的途径，数据库就是一种资源管理器。资源管理还应该具有管理事务提交或回滚的能力。

> Mysql的XA协议就是2PC协议的实现，一般来说2PC里面将集群中的节点分为协调者和参与者，而Mysql的XA中些调整对应的是事务管理器，参与者对应的是资源管理器

##### **第一阶段：投票阶段**

该阶段的主要目的在于打探数据库集群中的各个参与者是否能够正常的执行事务，具体步骤如下：

1. 协调者向所有的参与者发送事务执行请求，并等待参与者反馈事务执行结果。
2. 事务参与者收到请求之后，执行事务（锁定资源），但不提交，并记录事务日志。
3. 参与者将自己事务执行情况反馈给协调者，同时阻塞等待协调者的后续指令。

##### **第二阶段：事务提交阶段**

在第一阶段协调者的询盘之后，各个参与者会回复自己事务的执行情况，这时候存在三种可能：

1. 所有的参与者回复能够正常执行事务
2. 一个或多个参与者回复事务执行失败
3. 协调者等待超时

对于第一种情况，协调者将向所有的参与者发出提交事务的通知，具体步骤如下：

1. 协调者向各个参与者发送commit通知，请求提交事务。
2. 参与者收到事务提交通知之后，执行commit操作，然后释放占有的资源。
3. 参与者向协调者返回事务commit结果信息。

![事务提交时序图](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929121349rb46Ts.png)

![事务回滚时序图](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929121420Q4DNdq.png)

两阶段提交协议解决的是分布式系统中数据强一致性问题，其原理简单，易于实现，但是缺点也是显而易见的，主要缺点如下：

- **单点问题**：

协调者在整个两阶段提交过程中扮演着举足轻重的作用，一旦协调者所在服务器宕机，那么就会影响整个数据库集群的正常运行，比如在第二阶段中，如果协调者因为故障不能正常发送事务提交或回滚通知，那么参与者们将一直处于阻塞状态，整个数据库集群将无法提供服务。

- **同步阻塞**：两阶段提交执行过程中，所有的参与者都需要听从协调者的统一调度，期间处于阻塞状态而不能从事其他操作，这样效率及其低下。

- **数据不一致性**：两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能

  - 在事务提交阶段中，假设协调者发出了事务commit的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了commit操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。
  - 在事务提交阶段中，如果参与者向协调者反馈事务提交/回滚结果的时候，发生了网络波动，信息不可达，协调者需要向各个参与者查询事务执行结果

  > 2PC会有一定的几率各个参与者之间的状态不一致，这个时候需要参与者的接口具有幂等的特性，方便管理者进行重试，如果最后重试失败，需要有事务补偿的逻辑

### 三阶段提交（Three-phase commit）

也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。

与两阶段提交不同的是，三阶段提交有两个改动点。

1. 引入超时机制。同时在协调者和参与者中都引入超时机制。
2. 加入了一个CanCommit阶段，在真正执行事务（获取资源）之前先确认参与者是否能执行事务

也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有`CanCommit`、`PreCommit`、`DoCommit`三个阶段。

![三阶段提交](https://raw.githubusercontent.com/syllr/image/main/uPic/20211025101306z9A1S2.svg)

#### CanCommit阶段

协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应

> 1. **事务询问** 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。**注意此阶段并没有执行事务（没有加锁），执行事务是在PreCommit阶段**。
> 2. **响应反馈** 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No
> 3. CanCommit阶段的作用主要是对所有的参与者进行一次心跳检测，提前排除宕机，网络波动导致，资源被占用（资源被锁）导致的服务不可用（当然这一刻的节点的状态不代表下一刻节点的状态，还是会有服务不可用的情况，只是概率降低了）

#### PreCommit阶段

协调者根据参与者在CanCommit阶段的反应情况来决定是否可以记性事务的PreCommit操作。根据响应情况，有以下两种可能。

**假如协调者在CanCommit阶段从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行（这个和两阶段提交的预执行是一样的逻辑）。**

> **1.发送预提交请求** 协调者向参与者发送PreCommit请求，并进入Prepared阶段。
>
> **2.事务预提交** 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中，**对资源进行加锁**。
>
> **3.响应反馈** 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。

**假如在CanCommit阶段有任何一个参与者向协调者发送了No响应，或者等待超时之后(当协调者宕机或网络波动，参与者就会接收超时)，协调者都没有接到参与者的响应，那么就执行事务的中断。**

> **1.发送中断请求** 协调者向所有参与者发送abort请求。
>
> **2.中断事务** 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。

#### doCommit阶段

该阶段进行真正的事务提交，即对所有资源进行释放，也可以分为以下两种情况。

**执行提交**：

> **1.发送提交请求** 协调者在PreCommit阶段接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。
>
> **2.事务提交** 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。
>
> **3.响应反馈** 事务提交完之后，向协调者发送Ack响应。
>
> **4.完成事务** 协调者接收到所有参与者的ack响应之后，完成事务。

**中断事务** ：协调者在PreCommit阶段没有接收到某个参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。

> **1.发送中断请求** 协调者向所有参与者发送abort请求
>
> **2.事务回滚** 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。
>
> **3.反馈结果** 参与者完成事务回滚之后，向协调者发送ACK消息
>
> **4.中断事务** 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。

在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者abort请求时，会在等待超时之后，会继续进行事务的提交。

> 其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了）
>
> 所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。 

相对于2PC，3PC主要解决了协调者宕机之后事务阻塞的问题，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit，而不会一直持有事务资源并处于阻塞状态。

但是这种机制也会导致数据一致性问题，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。

### 2PC与3PC的区别

> 与两阶段提交不同的是，三阶段提交有两个改动点。
>
> 1. 引入超时机制。同时在协调者和参与者中都引入超时机制，在超时之后执行默认操作。
> 2. 加入了一个CanCommit阶段，在真正执行事务（获取资源）之前先确认参与者是否能执行事务

三阶段对于二阶段之所以能够引入超时机制执行默认操作，是因为加入了一个CanCommit阶段，CanCommit阶段相当于加入了一个Check机制

先通过CanCommit阶段 Check各个参与者的资源状态，即使在后面的阶段发生了网络波动，但是**他有理由相信：成功提交的几率很大。 **

三阶段是因为CanCommit阶段减少了提交失败的概率，当然二阶段提交也可以引入超时机制，在超时之后执行默认操作，但是这样成功提交的概率会低很多

三阶段和二阶段不能完全避免各个参与者之间数据不一致的情况，所以都需要事务补偿机制

## Mysql XA规范

DTP模型（ Distributed Transaction Processing），XA 规范约定的是 DTP 模型中 2 个模块（事务管理器和资源管理器）的通讯方式

<img src="https://static001.geekbang.org/resource/image/a0/93/a08794d4a09101fdc0789496a50db193.jpg" alt="img" style="zoom:67%;" />

DTP模型中各个模块的作用

* AP：应用程序（Aplication Program），一般指事务的发起者（比如数据库客户端或者访问数据库的程序），定义事务对应的操作（比如更新操作 UPDATE executed_table SET status = true WHERE id=100）。
* RM：资源管理器（Resource Manager），管理共享资源，并提供访问接口，供外部程序来访问共享资源，比如数据库，另外 RM 还应该具有事务提交或回滚的能力。
* TM：事务管理器（Transaction Manager），TM 是分布式事务的协调者。TM 与每个 RM 进行通信，协调并完成事务的处理。

应用程序访问、使用资源管理器的资源，并通过事务管理器的事务接口（TX interface）定义需要执行的事务操作，然后事务管理器和资源管理器会基于 XA 规范，执行二阶段提交协议。

XA 规范约定了事务管理器和资源管理器之间双向通讯的接口规范，并实现了二阶段提交协议：

<img src="https://raw.githubusercontent.com/syllr/image/main/uPic/20211025153641hpSlol.jpg" alt="img" style="zoom:67%;" />



## TCC

* TCC：TCC事务处理流程和 2PC 二阶段提交类似，不过 2PC通常都是在跨库的DB层面，而TCC本质就是一个应用层面的2PC

  ![在这里插入图片描述](https://raw.githubusercontent.com/syllr/image/main/uPic/20211009224251aYURSa.png)


# 分布式协议和算法

|            | 拜占庭容错 |          一致性          | 性能 | 可用性 |
| :--------: | :--------: | :----------------------: | :--: | :----: |
|    2PC     |     否     |  强一致性（内部一致性）  |  低  |   低   |
|    TCC     |     否     | 最终一致性（内部一致性） |  低  |   低   |
|   Paxos    |     否     | 最终一致性（外部一致性） | 中低 |   中   |
|    ZAB     |     否     |  强一致性（外部一致性）  |  中  |   中   |
|    Raft    |     否     |  强一致性（外部一致性）  |  中  |   中   |
|   Gossip   |     否     | 最终一致性（外部一致性） |  高  |   高   |
| Quorum NWR |     否     |  强一致性（外部一致性）  |  中  |   中   |
|    PBFT    |     是     |           N/A            |  低  |   中   |
|    POW     |     是     |           N/A            |  低  |   中   |

## 多数派

* 一个集群中所有节点的数量（副本数量）为N

> 副本数和节点数尽量保持一致，一个节点保存一个副本，因为如果一个节点保存多个副本，如果这个节点宕机，那这个节点的所有副本都不可用了，这样只是将数据冗余了，并没有提高可用性，数据冗余只是手段，提高可用性才是目的，所以我们可以认为一般情况下节点数和副本数是一致的

* 每次写命令写入成功的节点数为W
* 每次读命令读取的节点数为R

如果W大于N的一半，R大于N的一半，则W和N必定有交集，这就是多数派的读写，多数派读写的理论依据来源于一个集合的两个多数子集一定会有交集，因为同一个集合的两个不同子集的交集会有多种情况，这也是Paxos算法和NWR算法在没有满足多数派读的时候会有多种情况的原因

## Basic Paxos

### 场景化简

假设一个集群包含三个节点 A, B, C，提供只读 key-value 存储服务。只读 key-value 的意思是指，当一个 key 被创建时，它的值就确定下来了，且后面不能修改。

客户端 1 和客户端 2 同时试图创建一个 `X` 键。客户端 1 创建值为 `"leehao.me"` 的 `X` ，客户端 2 创建值为 `"www.leehao.me"` 的 `X`。在这种情况下，集群如何达成共识，实现各节点上 `X` 的值一致呢？

> 重点是要满足集群中所有节点数据的一致性

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/202109292233401Otxa0.png)

实现上面场景最直接能想到的是完全的主从同步方式，而完全的主从同步方式，需要等待集群中所有节点都设置数据之后才返回，延迟比较高，而且集群中任意一个节点不可用就会导致整个集群不可用，容错性低

### 多数派写

多数派写指的是只要一个集群中quorum个节点写入成功，就算写入成功（我们要求的场景是变量一旦被赋值就不会被修改，所以不用考虑后续修改的问题），在读取数据的时候需要多数派读，因为在多数派写的时候写入了quorum个节点，在读取的时候，也读取quorum个节点，这样一定可以读取到变量的值

> quorum个数必须大于集群节点数的一半
>
> 一个集合的任意两个quorum个元素的子集一定有有交集

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929221501e4Rjgl.png)

上面描述的多数派写并不涉及到并发的写，如果是并发的写呢，如下图，同时有两个客户端准备进行多数派写。

![3939ce9a3cd096a884a38bb9daf6d4bb](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929225132CUucEx.png)

* 要解决并发写，最简单的逻辑就是在进行多数派写之前先进行一次多数派读，如果数据已经被设置，那么就这次写就不设置值，如果数据没有被设置，那么这次写就设置值（这点很重要，Paxos算法的核心流程就是这个）

### Paxos 涉及的概念

在 Paxos 算法中，存在提议者（Proposer），接受者（Acceptor），学习者（Learner）三种角色，它们的关系如下：

- 提议者（Proposer）：提议一个值，用于投票表决，可以将上图客户端 1 和客户端 2 看作提议者。实际上，提议者更多是集群内的节点，这里为了演示的方便，将客户端 1 和 2 看作提议者，不影响 Paxos 算法的实质
- 接受者（Acceptor）：对每个提议的值进行投票，并存储接受的值，例如，上图集群内的节点 A、B、C
- 学习者（Learner）：被告知投票的结果，接受达成共识的值，不参与投票的过程，存储接受数据

需要指出的是，一个节点，既可以是提议者，也可以是接受者。

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223409QNKgOa.png)

在 Paxos 算法中，使用**提案**表示一个提议，提案包括提案编号和提议的值。接下来，我们使用 `[n, v]` 表示一个提案，其中，`n` 是提案编号，`v` 是提案的值。

在 Basic Paxos 中，集群中各个节点为了达成共识，需要进行 2 个阶段的协商，即准备（Prepare）阶段和接受（Accept）阶段。

### 准备阶段

假设客户端 1 的提案编号是 1，客户端 2 的提案编号为 5，并假设节点 A, B 先收到来自客户端 1 的准备请求，节点 C 先收到来自客户端 2 的准备请求。
客户端作为提议者，向所有的接受者发送包含提案编号的准备请求。注意在准备阶段，请求中不需要指定提议的值，只需要包含提案编号即可。

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223509APAQbQ.png)

接下来，节点 A，B 接收到客户端 1 的准备请求（提案编号为 1），节点 C 接收到客户端 2 的准备请求（提案编号为 5）。

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223604EUWgBN.png)

集群中各个节点在接收到第一个准备请求的处理：

- 节点 A, B：由于之前没有通过任何提案，所以节点 A，B 将返回“尚无提案”的准备响应，并承诺以后不再响应提案编号小于等于 1 的准备请求，不会通过编号小于 1 的提案
- 节点 C：由于之前没有通过任何提案，所以节点 C 将返回“尚无提案”的准备响应，并承诺以后不再响应提案编号小于等于 5 的准备请求，不会通过编号小于 5 的提案

接下来，当节点 A，B 接收到提案编号为 5 的准备请求，节点 C 接收到提案编号为 1 的准备请求：

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223621iAxfjx.png)

- 节点 A, B：由于提案编号 5 大于之前响应的准备请求的提案编号 1，且节点 A, B 都没有通过任何提案，故均返回“尚无提案”的响应，并承诺以后不再响应提案编号小于等于 5 的准备请求，不会通过编号小于 5 的提案
- 节点 C：由于节点 C 接收到提案编号 1 小于节点 C 之前响应的准备请求的提案编号 5 ，所以丢弃该准备请求，不作响应

### 接受阶段

Basic Paxos 算法第二阶段为接受阶段。当客户端 1，2 在收到大多数节点的准备响应之后会开始发送接受请求。

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223649R6D0gk.png)

- 客户端 1：客户端 1 接收到大多数的接受者（节点 A, B）的准备响应后，根据响应中的提案编号最大的提案的值，设置接受请求的值。由于节点 A, B 均返回“尚无提案”，即提案值为空，故客户端 1 把自己的提议值 `"leehao.me"` 作为提案的值，发送接受请求 `[1, "leehao.me"]`
- 客户端 2：客户端 2 接收到大多数接受者的准备响应后，根据响应中的提案编号最大的提案的值，设置接受请求的值。由于节点 A, B, C 均返回“尚无提案”，即提案值为空，故客户端 2 把自己的提议值 `"www.leehao.me"` 作为提案的值，发送接受请求 `[5, "www.leehao.me"]`

当节点 A, B, C 接收到客户端 1, 2 的接受请求时，对接受请求进行处理：

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/202109292237107F4kQ7.png)

- 节点 A, B, C 接收到接受请求 `[1, "leehao.me"]` ，由于提案编号 1 小于三个节点承诺可以通过的最小提案编号 5，所以提案 `[1, "leehao.me"]` 被拒绝
- 节点 A, B, C 接收到接受请求 `[5, "www.leehao.me"]`，由于提案编号 5 不小于三个节点承诺可以通过的最小提案编号 5 ，所以通过提案 `[5, "www.leehao.me"]`，即三个节点达成共识，接受 `X` 的值为 `"www.leehao.me"`

如果集群中还有学习者，当接受者通过一个提案，就通知学习者，当学习者发现大多数接受者都通过了某个提案，那么学习者也通过该提案，接受提案的值。

### 接受者存在已通过提案的情况

上面例子中，准备阶段和接受阶段均不存在接受者已经通过提案的情况。这里继续使用上面的例子，不过假设节点 A, B 已通过提案 `[5, "www.leehao.me"]`，节点 C 未通过任何提案。
增加一个新的提议者客户端 3，客户端 3 的提案为 `[9，"leehao"]` 。

接下来，客户端 3 执行准备阶段和接受阶段。

客户端 3 向节点 A, B, C 发送提案编号为 9 的准备请求：

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223759BfSj0K.png)

节点 A, B 接收到客户端 3 的准备请求，由于节点 A, B 已通过提案 `[5, "www.leehao.me"]`，故在准备响应中，包含此提案信息。
节点 C 接收到客户端 3 的准备请求，由于节点 C 未通过任何提案，故节点 C 将返回“尚无提案”的准备响应。

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223816iiXmVA.png)

客户端 3 接收到节点 A, B, C 的准备响应后，向节点 A, B, C 发送接受请求。这里需要特点指出，客户端 3 会根据响应中的提案编号最大的提案的值，设置接受请求的值，如果响应中没有值，才使用客户端3自己的值。由于在准备响应中，已包含提案 `[5, "www.leehao.me"]`，故客户端 3 将接受请求的提案编号设置为 9，提案值设置为 `"www.leehao.me"` 即接受请求的提案为 `[9, "www.leehao.me"]`：

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210929223855gHXjyR.png)

节点 A, B, C 接收到客户端 3 的接受请求，由于提案编号 9 不小于三个节点承诺可以通过的最小提案编号，故均通过提案 `[9, www.leehao.me]`。

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/202109292239131WLDTA.png)

概括来说，Basic Paxos 具有以下特点：

- Basic Paxos 通过二阶段方式来达成共识，即准备阶段和接受阶段
- Basic Paxos 除了达成共识功能，还实现了容错，在少于一半节点出现故障时，集群也能工作
- 提案编号大小代表优先级。对于提案编号，接受者提供三个承诺：
  - 如果准备请求的提案编号，小于等于接受者已经响应的准备请求的提案编号，那么接受者承诺不接受这个准备请求
  - 如果接受请求中的提案编号，小于接受者已经响应的准备请求的提案编号，那么接受者承诺不通过这个提案
  - 如果按受者已通过提案，那些接受者承诺会在准备请求的响应中，包含已经通过的最大编号的提案信息
- 提议者在处理Prepare请求返回的时候，会根据响应中的提案编号最大的提案的值，设置接受请求的值（相当于放弃了自己的值），如果所有返回中的值都为空，就用提议者自己的值

### Paxos解决的问题

* 如何处理首个提案

  > 1. Paxos提案的ID是通过时间戳+serverID生成的，是有序的
  >
  > 2. `如果准备请求的提案编号，小于等于接受者已经响应的准备请求的提案编号，那么接受者承诺不接受这个准备请求`
  >
  > 以上两点保证了在所有提案都还在Prepare阶段的时候无论经过多少次失败和重试，最后终会有一个或多个提案进入Accept阶段，并且最终一定是提案ID最大的提案被接受

* 如何解决冲突提案并达成共识

  > 1. `如果按受者已通过提案，那些接受者承诺会在准备请求的响应中，包含已经通过的最大编号的提案信息`
  > 2. `提议者在处理Prepare请求返回的时候，会根据响应中的提案编号最大的提案的值，设置接受请求的值（相当于放弃了自己的值），如果所有返回中的值都为空，就用提议者自己的值`
  >
  > 这两点说明了Paxos并不关心最终选出的值是哪一个，只需要保证所选出来的值是唯一的，

* 达成共识后如何处理新提案

  > 1. `如果按受者已通过提案，那些接受者承诺会在准备请求的响应中，包含已经通过的最大编号的提案信息`
  > 2. `提议者在处理Prepare请求返回的时候，会根据响应中的提案编号最大的提案的值，设置接受请求的值（相当于放弃了自己的值），如果所有返回中的值都为空，就用提议者自己的值`
  >
  > 当新的提案来到，因为集群中已经达成共识，由于新的提案也需要获取大多数节点支持，**`一个集合的两个包含多数元素的子集一定有交集`**，所以在新的提案进行准备阶段的时候一定会在Prepare响应中收到已经达成共识的值（因为要达成共识，一定是多数节点都accept，这些节点在新提案进行prepare查询的时候会把已经accept的值返回，新的提案会用响应中提案编号最大提案的值来代替自己的值）。所以新的提案会导致提案数变大，但是不会改变已经达成共识的值

* 当系统中已经有提案被Accept，如何处理新的提案

  > 这种情况下要看新的提案能够对哪些Acceptor进行prepare请求，
  >
  > * 如果刚好请求的Acceptor节点已经有接受的提案了，那么新的提案的值会被替换成它所请求的Acceptor中已经接受的最大提案的值
  > * 如果刚好请求的所有Acceptor都没有接受提案，那么新的提案的值可能会被大多数节点接受，变成共识（网络不出毛病的话）

### Paxos的问题

* 无法保证操作的顺序性

  > 当系统中已经有提案被Accept，如何处理新的提案
  >
  > > 这种情况下要看新的提案能够对哪些Acceptor进行prepare请求，
  > >
  > > * 如果刚好请求的Acceptor节点已经有接受的提案了，那么新的提案的值会被替换成它所请求的Acceptor中已经接受的最大提案的值
  > > * 如果刚好请求的所有Acceptor都没有接受提案，那么新的提案的值可能会被大多数节点接受，变成共识（网络不出毛病的话）
  > >
  > > 所以Paxos对于提案的值的设置是有随机性的，可能是新提案的值，也可能是已经被接受的提案的值，无法保证顺序性

## Multi Paxos

Basic Paxos 只能就单个值（Value）达成共识，一旦遇到为一系列的值实现共识的时候，它就不管用了

Multi-Paxos 算法是一个统称，它是指基于 Multi-Paxos 思想，通过多个 Basic Paxos 实例实现一系列值的共识的算法（比如 Chubby 的 Multi-Paxos 实现、Raft 算法等）

Basic Paxos 是通过二阶段提交来达成共识的。在第一阶段，也就是准备阶段，接收到大多数准备响应的提议者，才能发起接受请求进入第二阶段（也就是接受阶段）：

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/202110252318578j7Ank.jpg)

而如果我们直接通过多次执行 Basic Paxos 实例，来实现一系列值的共识，就会存在这样几个问题：

* 如果多个提议者同时提交提案，可能出现因为提案编号冲突，在准备阶段没有提议者接收到大多数准备响应，协商失败，需要重新协商。你想象一下，一个 5 节点的集群，如果 3 个节点作为提议者同时提案，就可能发生因为没有提议者接收大多数响应（比如 1 个提议者接收到 1 个准备响应，另外 2 个提议者分别接收到 2 个准备响应）而准备失败，需要重新协商。
* 2 轮 RPC 通讯（准备阶段和接受阶段）往返消息多、耗性能、延迟大。你要知道，分布式系统的运行是建立在 RPC 通讯的基础之上的，因此，延迟一直是分布式系统的痛点，是需要我们在开发分布式系统时认真考虑和优化的。

对于以上两个问题，可以通过引入领导者和优化 Basic Paxos 执行来解决。

* 领导者：通过引入领导者节点，领导者节点作为唯一提议者，这样就不存在多个提议者同时提交提案的情况，也就不存在提案冲突的情况了

* 优化 Basic Paxos 执行：当选出了领导者之后，可以直接省掉准备阶段，直接进入接受阶段

  > 也就是说，领导者节点上，序列中的命令是最新的，不再需要通过准备请求来发现之前被大多数节点通过的提案，领导者可以独立指定提案中的值。这时，领导者在提交命令时，可以省掉准备阶段，直接进入到接受阶段
  >
  > ![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20211025232347P6Y7Zh.jpg)

也就是说Multi Paxos算法通过引入领导者节点，解决了提案冲突的问题，而且可以利用领导者作为单点的特性，实现操作的顺序性

> 比如现在有两个命令需要设置X=1，Y=2，如果集群中每个节点都是接受提案，客户端需要的执行顺序为X=1，Y=2，因为网络的波动，可能在节点A中的执行顺序是X=1，Y=2，在节点B中的执行顺序是Y=2，X=1，Basic Paxos算法只能让集群中的节点对于单个值达成共识，不能保证命令的顺序性
>
> 而选出单个领导者之后可以在领导者内部对命令进行排序（和redis一样内部做成单线程的就可以了），然后再同步到集群中的其他节点，这样做的好处是，可以保证命令的顺序性，但是集群整体的写性能退化成为单机

所以Multi Paxos（Raft，ZAB）解决了这三个问题

* 在集群启动的时候选出一个主
* 主从同步
* 当主节点宕机，重新选出一个日志完整性最高的节点作为主节点

## Raft算法

Raft 算法属于 Multi-Paxos 算法，它是在兰伯特 Multi-Paxos 思想的基础上，做了一些简化和限制，比如增加了日志必须是连续的，只支持领导者、跟随者和候选人三种状态，在理解和算法实现上都相对容易许多，除此之外，Raft 算法是现在分布式系统开发首选的共识算法。

[Raft算法动态演示](http://thesecretlivesofdata.com/raft/)

从本质上说，Raft 算法是通过一切以领导者为准的方式，实现一系列值的共识和各节点日志的一致，而这种模型有其局限性

1. 强领导模型对于写功能基本退化单机性能，量大任然会出现性能瓶颈，适得其反。
2. 选举期间会集群将出现短暂不可用现象，影响时长与选举时间相关。
3. 对于读要实现强一致性读只能从Master节点读，从Flower节点读只能实现最终一致性

Raft算法主要解决三个问题，选举领导者（选主），领导者和追随者之间的复制，成员变更问题

### 成员身份

* 领导者：主要工作内容就是 3 部分，处理写请求、管理日志复制和不断地发送心跳信息，通知其他节点“我是领导者，我还活着，你们现在不要发起新的选举，找个新领导者来替代我。”

* 跟随者：就相当于普通群众，默默地接收和处理来自领导者的消息，当等待领导者心跳信息超时的时候，就主动站出来，推荐自己当候选人。
* 候选人：候选人将向其他节点发送请求投票（RequestVote）RPC 消息，通知其他节点来投票，如果赢得了大多数选票，就晋升当领导者。

需要注意的是，Raft 算法是强领导者模型，集群中只能有一个领导者。

### 领导者选举过程

首先，在初始状态下，集群中所有的节点都是跟随者的状态。

<img src="https://raw.githubusercontent.com/syllr/image/main/uPic/20210930094711xPQKF9.jpg" alt="img" style="zoom:50%;" />

Raft 算法实现了随机超时时间的特性。也就是说，每个节点等待领导者节点心跳信息的超时时间间隔是随机的。

通过上面的图片你可以看到，集群中没有领导者，而节点 A 的等待超时时间最小（150ms），它会最先因为没有等到领导者的心跳信息，发生超时。这个时候，节点 A 就增加自己的任期编号，并推举自己为候选人，先给自己投上一张选票，然后向其他节点发送请求投票 RPC 消息，请它们选举自己为领导者。

<img src="https://static001.geekbang.org/resource/image/aa/9c/aac5704d69f142ead5e92d33f893a69c.jpg" alt="img" style="zoom:50%;" />

如果其他节点接收到候选人 A 的请求投票 RPC 消息，在编号为 1 的这届任期内，也还没有进行过投票，那么它将把选票投给节点 A，并增加自己的任期编号。

<img src="https://static001.geekbang.org/resource/image/a4/95/a4bb6d1fa7c8c48106a4cf040b7b1095.jpg" alt="img" style="zoom:50%;" />

如果候选人在选举超时时间内赢得了大多数的选票，那么它就会成为本届任期内新的领导者。

<img src="https://static001.geekbang.org/resource/image/ff/2c/ffaa3f6e9e87d6cea2a3bfc29647e22c.jpg" alt="img" style="zoom:50%;" />

节点 A 当选领导者后，他将周期性地发送心跳消息，通知其他服务器我是领导者，阻止跟随者发起新的选举，篡权。

<img src="https://static001.geekbang.org/resource/image/0a/91/0a626f52c2e2a147c59c862b148be691.jpg" alt="img" style="zoom:50%;" />

#### 节点间如何通讯？

在 Raft 算法中，服务器节点间的沟通联络采用的是远程过程调用（RPC），在领导者选举中，需要用到这样两类的 RPC：

1. 请求投票（RequestVote）RPC，是由候选人在选举期间发起，通知各节点进行投票；
2. 日志复制（AppendEntries）RPC，是由领导者发起，用来复制日志和提供心跳消息。

#### 什么是任期？

Raft 算法中的领导者也是有任期的，每个任期由单调递增的数字（任期编号）标识，比如节点 A 的任期编号是 1。任期编号是随着选举的举行而变化的，这是在说下面几点。

1. 跟随者在等待领导者心跳信息超时后，推举自己为候选人时，会增加自己的任期号，比如节点 A 的当前任期编号为 0，那么在推举自己为候选人时，会将自己的任期编号增加为 1。
2. 如果一个服务器节点，发现自己的任期编号比其他节点小，那么它会更新自己的编号到较大的编号值。比如节点 B 的任期编号是 0，当收到来自节点 A 的请求投票 RPC 消息时，因为消息中包含了节点 A 的任期编号，且编号为 1，那么节点 B 将把自己的任期编号更新为 1。
3. 在 Raft 算法中约定，如果一个候选人或者领导者，发现自己的任期编号比其他节点小，那么它会立即恢复成跟随者状态。比如分区错误恢复后，任期编号为 3 的领导者节点 B，收到来自新领导者的，包含任期编号为 4 的心跳消息，那么节点 B 将立即恢复成跟随者状态。
4. 还约定如果一个节点接收到一个包含较小的任期编号值的请求，那么它会直接拒绝这个请求。比如节点 C 的任期编号为 4，收到包含任期编号为 3 的请求投票 RPC 消息，那么它将拒绝这个消息。

#### 选举有哪些规则

1. 领导者周期性地向所有跟随者发送心跳消息（即不包含日志项的日志复制 RPC 消息），通知大家我是领导者，如果有节点接受到投票请求，如果这个节点和主节点心跳正常，则这个节点会拒绝投票，这样可以阻止跟随者重新投票获取大多数选票**篡权**

2. 如果在指定时间内，跟随者没有接收到来自领导者的消息，那么它就认为当前没有领导者，推举自己为候选人，发起领导者选举。

3. 在一次选举中，赢得大多数选票的候选人，将晋升为领导者。

4. 在一个任期内，领导者一直都会是领导者，直到它自身出现问题（比如宕机），或者因为网络延迟，其他节点发起一轮新的选举。

5. 在一次选举中，**每一个服务器节点最多会对一个任期编号投出一张选票，并且按照“先来先服务”的原则进行投票**。比如节点 C 的任期编号为 3，先收到了 1 个包含任期编号为 4 的投票请求（来自节点 A），然后又收到了 1 个包含任期编号为 4 的投票请求（来自节点 B）。那么节点 C 将会把唯一一张选票投给节点 A，当再收到节点 B 的投票请求 RPC 消息时，对于编号为 4 的任期，已没有选票可投了。

   <img src="https://static001.geekbang.org/resource/image/33/84/3373232d5c10813c7fc87f2fd4a12d84.jpg" alt="img" style="zoom:50%;" />

6. **日志完整性高的跟随者**（也就是最后一条日志项对应的任期编号值更大，索引号更大），拒绝投票给日志完整性低的候选人。比如节点 B 的任期编号为 3，节点 C 的任期编号是 4，节点 B 的最后一条日志项对应的任期编号为 3，而节点 C 为 2，那么当节点 C 请求节点 B 投票给自己时，节点 B 将拒绝投票。

   > 这样可以保证选出来的新leader的日志完整性最高，因为日志完整性不高的候选人不会得到超过半数选票

   <img src="https://static001.geekbang.org/resource/image/99/6d/9932935b415e37c2ca758ab99b34f66d.jpg" alt="img" style="zoom:67%;" />

选举是跟随者发起的，推举自己为候选人；大多数选票是指集群成员半数以上的选票；大多数选票规则的目标，是为了保证在一个给定的任期内最多只有一个领导者。

其实在选举中，除了选举规则外，我们还需要避免一些会导致选举失败的情况，比如同一任期内，多个候选人同时发起选举，导致选票被瓜分，选举失败。那么在 Raft 算法中，如何避免这个问题呢？答案就是随机超时时间。

1. 跟随者等待领导者心跳信息超时的时间间隔，是随机的；
2. 如果候选人在一个随机时间间隔内，没有赢得过半票数，那么选举无效了，然后候选人发起新一轮的选举，也就是说，等待选举超时的时间间隔，是随机的。

### 如何复制日志

副本数据是以日志的形式存在的，日志是由日志项组成

日志项究竟是什么样子呢？其实，日志项是一种数据格式，它主要包含用户指定的数据，也就是指令（Command），还包含一些附加信息，比如索引值（Log index）、任期编号（Term）。

<img src="https://static001.geekbang.org/resource/image/d5/6d/d5c7b0b95b4289c10c9e0817c71f036d.jpg" alt="img" style="zoom: 67%;" />

* 指令：一条由客户端请求指定的、状态机需要执行的指令。你可以将指令理解成客户端指定的数据。
* 索引值：日志项对应的整数索引值。它其实就是用来标识日志项的，是一个连续的、单调递增的整数号码。
* 任期编号：创建这条日志项的领导者的任期编号。

一届领导者任期，往往有多条日志项。而且**日志项的索引值是连续的**（这一点和ZAB一样，保证了日志的顺序性），Paxos算法不要求日志是连续的（所以Psxos无法保证顺序性），在 Raft 中，日志不仅是数据的载体，日志的完整性还影响领导者选举的结果。也就是说，日志完整性最高的节点才能当选领导者。

#### 复制流程

<img src="https://static001.geekbang.org/resource/image/b8/29/b863dc8546a78c272c965d6e05afde29.jpg" alt="img" style="zoom:67%;" />

1. 接收到客户端请求后，领导者基于客户端请求中的指令，创建一个新日志项，并附加到本地日志中

2. 领导者通过日志复制 RPC，将新的日志项复制到其他的服务器
   1. 领导者只是把日志复制给追随者，追随者接收成功就算成功，不会等到追随者把日志执行才返回
   2. 追随者接收日志的时候会判断当前已经复制过的最大日志索引，如果领导者发送过来同步的日志索引没有和最大日志索引连续，则返回失败
   3. 领导者在复制日志给追随者的时候如果追随者返回失败，领导者会发送当前日志的上一条日志，直至成功
   4. 领导者第一次发送成功的日志并不是最新的日志，就会连续发送日志直到最新的日志
   
3. 当领导者将日志项，成功复制到大多数的服务器上的时候，领导者会将这条日志项应用到它的状态机中（领导者的日志就提交了）

4. 领导者将执行的结果返回给客户端

5. **当跟随者接收到心跳信息，或者新的日志复制 RPC 消息后（这两个消息都会带着领导者最新的已提交日志的事务ID）**，如果跟随者发现领导者已经提交了某条日志项，而它还没应用，那么跟随者就将这条日志项应用到本地的状态机中

   > Raft的复制模式其实是一个一阶段提交，领导者把日志复制到跟随者上面就直接返回，并且领导者在将复制到大多数节点之后就直接提交了，而跟随者的日志的提交分两种情况
   >
   > * 跟随者接收到下一次领导者的日志复制请求时（请求中包含当前领导者的日志复制进度），跟随者会检查自己当前日志的提交进度，如果落后进度的日志未提交，则会直接提交
   > * 跟随者接收到领导者的心跳请求时（请求中包含当前领导者的日志复制进度），会检查自己当前日志的提交进度，如果落后进度的日志未提交，则会直接提交
   >
   > 可以看出Raft跟随者的日志提交是通过
   >
   > 1. Job（利用心跳请求实现）
   > 2. 懒提交（下一次复制请求的时候才提交上一次的日志）
   >
   > 因为Raft的节点在进入选举状态的时候，不会统计未提交的日志，所以必然会出现数据丢失的情况，比如当一个领导者写入数据commit之后，跟随者还没有来得及收到心跳或者下一次的领导者请求，这个时候领导者宕机，跟随者进入选举状态，还没有来得及的commit的日志会丢失

### 成员变更的问题

在集群中进行成员变更的最大风险是，可能会同时出现 2 个领导者。比如在进行成员变更时，节点 A、B 和 C 之间发生了分区错误，节点 A、B 组成旧配置中的“大多数”，也就是变更前的 3 节点集群中的“大多数”，那么这时的领导者（节点 A）依旧是领导者。

> 原来集群中有ABC三个节点，后来增加DE两个节点，在出现网络分区时AB一区，CED一区，因为集群中配置更新的不及时，AB节点会认为集群中只有3个节点（quorum是2），CED会认为集群中有5个节点（quorum是3）
>
> * 如果AB节点发生选举，因为quorum是2，AB节点同时选举A成为leader
> * CED节点发生选举，因为quorum是3，CED同时选举C成为leader
>
> 上面这种情况就会出现有两个领导者的情况

#### 单节点变更解决成员变更问题

单节点变更，就是通过一次变更一个节点实现成员变更。如果需要变更多个节点，那你需要执行多次单节点变更。比如将 3 节点集群扩容为 5 节点集群，这时你需要执行 2 次单节点变更，先将 3 节点集群变更为 4 节点集群，然后再将 4 节点集群变更为 5 节点集群，就像下图的样子。

<img src="https://static001.geekbang.org/resource/image/7e/55/7e2b1caf3c68c7900d6a7f71e7a3a855.jpg" alt="img" style="zoom: 50%;" />

在单节点变更的情况下，一次只新增一个节点，**不管旧的集群配置是怎么组成的，旧配置的“大多数”和新配置的“大多数”都会有一个节点是重叠的**，所以无论网络怎么分区，都不会选出两个领导者

> 原来集群中有ABC三个节点，后来增加C这个节点，老集群配置的quorum是2，新集群中节点数量为4，所以quorum是3，要选出两个leader，就必须一个分区里面有2个或以上节点，另一个分区里面有3个或以上节点，而集群中一共只有4个节点，所以不可能选出两个leader

## Gossip协议

Gossip 协议，顾名思义，就像流言蜚语一样，利用一种随机、带有传染性的方式，将信息传播到整个网络中，并在一定时间内，使得系统内的所有节点数据一致（最终一致性）。

### Gossip的消息传播方式

Gossip采用所谓谣言传播的方式在节点中传播消息，它指的是当一个节点有了新数据后，这个节点变成活跃状态，并周期性地联系其他节点向其发送新数据，直到所有的节点都存储了该新数据

![谣言传播过程](https://raw.githubusercontent.com/syllr/image/main/uPic/20211029115851a0QSvk.gif)

> 谣言传播可以很好的解决当节点**动态变化**时分布式环境中各个节点数据一致性的问题，但是网络传播具有不确定性，总是会有节点向别的节点发送数据的时候发送失败（经过重试之后也失败），为了确保最终一致性，还需要一种兜底的逻辑，这就是一致性修复

### Gossip的一致性修复方式

反熵是一种通过异步修复实现最终一致性的方法。常见的最终一致性系统（比如 Cassandra），都实现了反熵功能。

反熵指的是集群中的节点，每隔段时间就随机选择某个其他节点，然后通过互相交换自己的所有数据来消除两者之间的差异，实现数据的最终一致性，在实现反熵的时候主要有推、拉和推拉三种方式

拿ZK举个例子，如果使用推拉的方式进行反墒，就是选取两个节点A和B，A和B交换两者的数据，通过版本号来判断哪个节点的数据是最新的数据，然后各个节点将自己内部老版本的数据更新到新的版本

> 虽然反熵很实用，但是执行反熵时，相关的节点必须都是已知的，而且节点数量不能太多，其实如果只用反墒也是能实现最终一致性的，但是考虑到两点
>
> * 因为反熵需要做一致性对比，很消耗系统性能
> * 如果是一个**动态变化**或节点数比较多的分布式环境（比如在 DevOps 环境中检测节点故障，并动态维护集群节点状态），这时反熵就不适用了
>
> 反墒更多的只是用来进行最终一致性的兜底

## ZAB协议

ZAB协议是专门为zookeeper实现分布式协调功能而设计。zookeeper主要是根据ZAB协议是实现分布式系统数据一致性，ZAB和Raft一样，都是强领导模型，本质上讲，zab和raft都是通过强领导者模型实现就多值达成共识的

ZAB协议最核心的设计就是**在分布式的情况下保证操作的顺序性**，与之相对应的Paxos协议虽然能保证达成共识后的值不再改变，但它不关心达成共识的值是什么，也无法保证各值（也就是操作）的顺序性，所以zookeeper专门设计了ZAB协议。

### 选举流程

#### 成员身份

* 领导者（Leader）： 作为主（Primary）节点，在同一时间集群只会有一个领导者。需要你注意的是，所有的写请求都必须在领导者节点上执行。
* 跟随者（Follower）：作为备份（Backup）节点， 集群可以有多个跟随者，它们会响应领导者的心跳，并参与领导者选举和提案提交的投票。需要你注意的是，跟随者可以直接处理并响应来自客户端的读请求，但对于写请求，跟随者需要将它转发给领导者处理。
* 观察者（Observer）：作为备份（Backup）节点，类似跟随者，但是没有投票权，也就是说，观察者不参与领导者选举和提案提交的投票。你可以对比着 Paxos 中的学习者来理解。

虽然 ZAB 支持 3 种成员身份，但是它定义了 4 种成员状态。

* LOOKING：选举状态，该状态下的节点认为当前集群中没有领导者，会发起领导者选举。
* FOLLOWING ：跟随者状态，意味着当前节点是跟随者。
* LEADING ：领导者状态，意味着当前节点是领导者。
* OBSERVING： 观察者状态，意味着当前节点是观察者。

为什么多了一种成员状态呢？这是因为 ZAB 支持领导者选举，在选举过程中，涉及了一个过渡状态（也就是选举状态）。

假设投票信息的格式是<proposedLeader, proposedEpoch, proposedLastZxid, node> ，其中：

* proposedLeader，节点提议的，领导者的集群 ID，也就是在集群配置（比如 myid 配置文件）时指定的 ID。
* proposedEpoch，节点提议的，领导者的任期编号。
* proposedLastZxid，节点提议的，领导者的事务标识符最大值（也就是最新提案的事务标识符）。
* node，投票的节点，比如节点 B。

假设一个 ZooKeeper 集群，由节点 A、B、C 组成，其中节点 A 是领导者，节点 B、C 是跟随者（为了方便演示，假设 epoch 分别是 1 和 1，lastZxid 分别是 101 和 102，集群 ID 分别为 2 和 3）。那么如果节点 A 宕机了，会如何选举呢？

<img src="https://raw.githubusercontent.com/syllr/image/main/uPic/20210930144944izkeHf.jpg" alt="img" style="zoom:67%;" />

首先，当跟随者检测到连接领导者节点的读操作等待超时了，跟随者会变更节点状态，将自己的节点状态变更成 LOOKING，然后发起领导者选举

<img src="https://static001.geekbang.org/resource/image/2b/23/2bc48cf03938cc37d3513239da847c23.jpg" alt="img" style="zoom:67%;" />

接着，每个节点会创建一张选票，这张选票是投给自己的，也就是说，节点 B、C 都“自告奋勇”推荐自己为领导者，并创建选票 <2, 1, 101, B> 和 <3, 1, 102, C>，然后各自将选票发送给集群中所有节点，也就是说，B 发送给 B、C，C 也发送给 B、C。

一般而言，节点会先接收到自己发送给自己的选票（因为不需要跨节点通讯，传输更快），也就是说，B 会先收到来自 B 的选票，C 会先收到来自 C 的选票：

![img](https://raw.githubusercontent.com/syllr/image/main/uPic/20210930151847K0Usnl.jpg)

需要你注意的是，集群的各节点收到选票后，为了选举出数据最完整的节点，对于每一张接收到选票，节点都需要进行领导者 PK，也就将选票提议的领导者和自己提议的领导者进行比较，找出更适合作为领导者的节点，约定的规则如下：

1. 优先检查任期编号（Epoch），任期编号大的节点作为领导者；
2. 如果任期编号相同，比较事务标识符的最大值，值大的节点作为领导者；
3. 如果事务标识符的最大值相同，比较集群 ID，集群 ID 大的节点作为领导者。

如果选票提议的领导者，比自己提议的领导者，更适合作为领导者，那么节点将调整选票内容，推荐选票提议的领导者作为领导者。

当节点 B、C 接收到的选票后，因为选票提议的领导者与自己提议的领导者相同，所以，领导者 PK 的结果，是不需要调整选票信息，那么节点 B、C，正常接收和保存选票就可以了。

<img src="https://static001.geekbang.org/resource/image/4c/96/4c0546cfe2dbc11ed99cab414ec12e96.jpg" alt="img" style="zoom:67%;" />

接着节点 B、C 分别接收到来自对方的选票，比如 B 接收到来自 C 的选票，C 接收到来自 B 的选票：

<img src="https://static001.geekbang.org/resource/image/5d/d5/5d3fe6b30854490faf096826c62df5d5.jpg" alt="img" style="zoom:67%;" />

对于 C 而言，它提议的领导者是 C，而选票（<2, 1, 101, B>）提议的领导者是 B，因为节点 C 的任期编号与节点 B 相同，但节点 C 的事务标识符的最大值比节点 B 的大，那么，按照约定的规则，相比节点 B，节点 C 更适合作为领导者，也就是说，节点 C 不需要调整选票信息，正常接收和保存选票就可以了。

但对于对于节点 B 而言，它提议的领导者是 B，选票（<3, 1, 102, C>）提议的领导者是 C，因为节点 C 的任期编号与节点 B 相同，但节点 C 的事务标识符的最大值比节点 B 的大，那么，按照约定的规则，相比节点 B，节点 C 应该作为领导者，所以，节点 B 除了接收和保存选票信息，还会更新自己的选票为 <3, 1, 102, B>，也就是推荐 C 作为领导者，并将选票重新发送给节点 B、C：

<img src="https://static001.geekbang.org/resource/image/bf/77/bf70a4f1e9c28de2bbd3c134193ae577.jpg" alt="img" style="zoom:67%;" />

接着，当节点 B、C 接收到来自节点 B，新的选票时，因为这张选票（<3, 1, 102, B>）提议的领导者，与他们提议的领导者是一样的，都是节点 C，所以，他们正常接收和存储这张选票，就可以。

<img src="https://static001.geekbang.org/resource/image/04/7c/04ec7fdf75160571c8689f040292d57c.jpg" alt="img" style="zoom:67%;" />

最后，因为此时节点 B、C 提议的领导者（节点 C）赢得大多数选票了（2 张选票），那么，节点 B、C 将根据投票结果，变更节点状态，并退出选举。比如，因为当选的领导者是节点 C，那么节点 B 将变更状态为 FOLLOWING，并退出选举，而节点 C 将变更状态为 LEADING，并退出选举。

<img src="https://static001.geekbang.org/resource/image/a9/03/a9113e0d2653c77ddc41778510784003.jpg" alt="img" style="zoom:67%;" />

**ZAB 本质上是通过“见贤思齐，相互推荐”的方式来选举领导者的。也就说，根据领导者 PK，节点会重新推荐更合适的领导者，最终选举出了大多数节点中数据最完整的节点。**使用元组{选举轮次、事务ID、节点MyID}，加上“见贤思齐”的选举行为，一定可以选出唯一的Leader，不会出现像Raft那样选票被瓜分，选举失败的问题。

> Raft在选举的时候采用的是每次都把票投给第一个过来请求的候选人（还会有日志完整性的判断），这种方式会出现选票被瓜分，没有一个候选人获得半数以上票的情况，所以Raft采用了Random时间重试的机制
>
> ZAB的方式每次一定都会选出一个唯一Leader，但是每次投票发送的信息量也多，实现比较复杂

<img src="https://static001.geekbang.org/resource/image/83/57/83c37bc2a25a4e892cf1a5c3a2c6c457.jpg" alt="img" style="zoom:67%;" />

### 读写操作

#### 场景思考

假如节点 A、B、C 组成的一个分布式集群，我们要设计一个算法，来保证指令（比如 X、Y）执行的顺序性，比如，指令 X 在指令 Y 之前执行，那么我们该如何设计这个算法呢？

<img src="https://static001.geekbang.org/resource/image/55/c9/55dc6f6bf822db027858b8b4fdb89cc9.jpg" alt="img" style="zoom:67%;" />

Raft 可以实现操作的顺序性啊，为什么 ZooKeeper 不用 Raft 呢？

这个问题其实比较简单，因为 Raft 出来的比较晚，直到 2013 年才正式提出，在 2007 年开发 ZooKeeper 的时候，还没有 Raft 呢。

#### ZAB 是如何实现操作的顺序性的

我们现在的需求是要先设置X再设置Y

在 ZAB 中，写操作必须在主节点（比如节点 A）上执行。如果客户端访问的节点是备份节点（比如节点 B），它会将写请求转发给主节点。如图所示：

<img src="https://static001.geekbang.org/resource/image/77/6f/770c39b4ea339799bc3ca4a0b0d8266f.jpg" alt="img" style="zoom:67%;" />

接着，当主节点接收到写请求后，它会基于写请求中的指令（也就是 X，Y），来创建一个提案（Proposal），并使用一个唯一的 ID 来标识这个提案。这里我说的唯一的 ID 就是指事务标识符（Transaction ID，也就是 zxid），就像下图的样子。

<img src="https://static001.geekbang.org/resource/image/d0/9b/d0063fa9275ce0a114ace27db326d19b.jpg" alt="img" style="zoom:67%;" />

从图中你可以看到，X、Y 对应的事务标识符分别为 <1, 1> 和 <1, 2>，事务标识符是 64 位的 long 型变量，有任期编号 epoch 和计数器 counter 两部分组成（为了形象和方便理解，epoch 直接翻译成任期编号），格式为 ，高 32 位为任期编号，低 32 位为计数器：

* 任期编号，就是创建提案时领导者的任期编号，需要你注意的是，当新领导者当选时，任期编号递增，计数器被设置为零。比如，前领导者的任期编号为 1，那么新领导者对应的任期编号将为 2。
* 计数器，就是具体标识提案的整数，需要你注意的是，每次领导者创建新的提案时，计数器将递增。比如，前一个提案对应的计数器值为 1，那么新的提案对应的计数器值将为 2。

这样做的目的是为了保证事务标识符必须按照顺序、唯一标识一个提案，也就是说，事务标识符必须是唯一的、递增的。

> Raft是通过维护一个全局的计数器，来保证所有事务id都是连续的，其实两者本质都一样都是为了让事务有序

在创建完提案之后，主节点会基于 TCP 协议，并按照顺序将提案广播到其他节点。这样就能保证先发送的消息，会先被收到，保证了消息接收的顺序性。

<img src="https://static001.geekbang.org/resource/image/e5/96/e525af146900c892e0c002affa77d496.jpg" alt="img" style="zoom:67%;" />

X 一定在 Y 之前到达节点 B、C。

然后，当主节点接收到指定提案的“大多数”的确认响应后，该提案将处于提交状态（Committed），主节点会通知备份节点提交该提案。

> 在 Leader 和 Follwer 之间还有一个消息队列，用来解耦他们之间的耦合，解除同步阻塞。
>
> Follwer发生故障，然后重新连接之后，当主节点发送日志给Follwer的时候，Follwer会将发过来的日志的事务ID和自身已经接受过的最大的事务ID做对比，比对结果要么回滚，要么和 Leader 同步。

<img src="https://static001.geekbang.org/resource/image/1d/19/1d3950b6d91845789cce1f0569969419.jpg" alt="img" style="zoom:67%;" />

需要注意的是，主节点提交提案是有顺序性的。主节点根据事务标识符大小，按照顺序提交提案，如果前一个提案未提交，此时主节点是不会提交后一个提案的。也就是说，指令 X 一定会在指令 Y 之前提交。

最后，主节点返回执行成功的响应给节点 B，节点 B 再转发给客户端。你看，这样我们就实现了操作的顺序性，保证了指令 X 一定在指令 Y 之前执行。

当写操作执行完后，接下来可能需要执行读操作了。需要注意的是，为了提升读并发能力，Zookeeper 提供的是最终一致性，也就是读操作可以在任何节点上执行，客户端会读到旧数据：

<img src="https://static001.geekbang.org/resource/image/d4/1c/d405381e5fad12730149baa4fae63e1c.jpg" alt="img" style="zoom:67%;" />

如果客户端必须要读到最新数据，怎么办呢？Zookeeper 提供了一个解决办法，那就是 sync 命令。你可以在执行读操作前，先执行 sync 命令，这样客户端就能读到最新数据了。

> 还有一种做法是，让客户端一次读取性读取quorum个节点，然后从所有节点中的数据中选择事务标识符最大版本的数据（多数派读的逻辑）
>

### Zookeeper处理写请求

在 ZooKeeper 中，写请求是必须在领导者上处理，如果跟随者接收到了写请求，它需要将写请求转发给领导者，当写请求对应的提案被复制到大多数节点上时，领导者会提交提案，并通知跟随者提交提案。而读请求可以在任何节点上处理，也就是说，ZooKeeper 实现的是最终一致性。

在 ZooKeeper 中，与领导者“失联”的节点，是不能处理读写请求的。比如，如果一个跟随者与领导者的连接发生了读超时，设置了自己的状态为 LOOKING，那么此时它既不能转发写请求给领导者处理，也不能处理读请求，只有当它“找到”领导者后，才能处理读写请求。

当发生分区故障了，C 与 A（领导者）、B 网络不通了，那么 C 将设置自己的状态为 LOOKING，此时在 C 节点上既不能执行读操作，也不能执行写操作

<img src="https://static001.geekbang.org/resource/image/22/ad/22dfaa624590885c4b8406deb445afad.jpg" alt="img" style="zoom:67%;" />

其次，当大多数节点进入到广播阶段的时候，领导者才能提交提案，因为提案提交，需要来自大多数节点的确认。

最后，写请求只能在领导者节点上处理，**所以 ZooKeeper 集群写性能约等于单机**。而读请求是可以在所有的节点上处理的，所以，读性能是能水平扩展的。也就是说，你可以通过分集群的方式来突破写性能的限制，并通过增加更多节点，来扩展集群的读性能。

### 成员发现和数据同步

当选出新的Leader之后Leader会给集群中各个Follower通过LEADERINFO和FOLLOWINFO命令交换信息

1. 当集群中选出Leader之后，所有的Follower做的第一件事就是使用FOLLOWERINFO命令把自己的epoch和myid信息发送给Leader

2. 当收到半数以上的Follwer信息之后，会从这些Follwer发送的epoch中选出最大的，然后+1作为自己的新的epoch，然后通过LEAERINFO命令发送给所有Follower

3. Follower接收到新的epoch之后将新的epoch记录下来，然后使用ACKEPOCH回复Leader并带上自己这边最大的zxid（事务id），表示刚刚的LEADERINFO收到了

4. Leader收到Follwer发送的ACKEPOCH之后，会根据各个Follower的信息给出不同的同步策略：

   1. DIFF，如果 Follower 的记录和 Leader 的记录相差的不多，使用增量同步的方式将一个一个写请求发送给 Follower
   2. TRUNC，这个情况的出现代表 Follower 的 zxid 是领先于当前的 Leader 的（可能是以前的 Leader），需要 Follower 自行把多余的部分给截断，降级到和 Leader 一致
   3. SNAP，如果 Follower 的记录和当前 Leader 相差太多，Leader 直接将自己的整个内存数据发送给 Follower

   > 当follwer宕机，重新上线之后也会进行上面的步骤1，3，4，来进行同步

### ZooKeeper 实现写操作

<img src="https://static001.geekbang.org/resource/image/c7/8a/c77c241713b154673e15083fd063428a.jpg" alt="img" style="zoom:67%;" />

当领导者接收到写请求之后和跟随者同步数据的过程其实是一个2PC（不同点是2PC需要所有节点都返回成功，ZAB只需要大多数节点返回成功，但是都要经过两阶段的请求）

节点退出跟随者状态时（也就是在进入选举前），所有未提交的提案都会被提交。所以当领导者在处理请求的时候宕机，**所以新选出来的领导者，可能会有以前领导者未提交的日志，也可能没有，这之间存在不确定性**

> 所以ZAB在数据同步阶段才会有DIFF，TRUNC，SNAP三种策略，因为会存在选出来的新领导者中的数据没有集群中其他节点数据全的情况（当然这种情况对于客户端来说是正常的，因为客户端在执行写命令的时候领导者宕机，客户端也不知道请求到底是执行成功还是失败）

### ZooKeeper处理读请求

<img src="https://static001.geekbang.org/resource/image/f4/6d/f405d2a81f374e6e63b49c469506f26d.jpg" alt="img" style="zoom:67%;" />



## Raft对比ZAB

Raft 算法和 ZAB 协议很类似，比如主备模式（也就是领导者、跟随者模型）、日志必须是连续的、以领导者的日志为准来实现日志一致等等。

* 领导者选举：ZAB 采用的“见贤思齐、相互推荐”的快速领导者选举（Fast Leader Election），Raft 采用的是“一张选票、先到先得”的自定义算法。Raft 的领导者选举，需要通讯的消息数更少，选举也更快。

  > 对于节点的日志完整性的判断，ZAB是会变更自己手里的选票，而Raft则是如果候选者的日志完整性没有当前节点高，则当前节点拒绝投票

* 日志复制：Raft 和 ZAB 相同，都是以领导者的日志为准来实现日志一致，而且日志必须是连续的，也必须按照顺序提交。

* 读操作和一致性：ZAB 的设计目标是操作的顺序性，在 ZooKeeper 中默认实现的是最终一致性，读操作可以在任何节点上执行；而 Raft 的设计目标是强一致性（也就是线性一致性），所以 Raft 更灵活，Raft 系统既可以提供强一致性，也可以提供最终一致性。

* 写操作：Raft 和 ZAB 相同，写操作都必须在领导者节点上处理。

  > Zookeeper通过Leader来主导写操作，保证了顺序一致性。当一半以上的节点返回已写入，就返回客户端已写入，但是这时候只是部分节点写入，有的节点可能还没有同步上数据，所以读取备份节点可能不是最新的。同时Zookeeper的**单一视图特征**，保证客户端看到的数据不会比在之前服务器上所看到的更老。

* 成员变更：Raft 和 ZAB 都支持成员变更，其中 ZAB 以动态配置（dynamic configuration）的方式实现的。那么当你在节点变更时，不需要重启机器，集群是一直运行的，服务也不会中断。

* 数据同步：相比 ZAB，Raft 的设计更为简洁，

  *  Raft 没有引入类似 ZAB 的成员发现和数据同步阶段，而是当节点发起选举时，递增任期编号，在选举结束后，广播心跳，直接建立领导者关系，然后向各节点同步日志，来实现数据副本的一致性。ZAB则是当跟随者和领导者建立联系的时候先进行数据同步，确保每一个跟随者和领导者的日志数据是一样的
  * ZAB在进行数据同步的时候使用的是类似于两阶段提交的策略，需要两轮请求，而raft使用的是一阶段提交配合惰性提交+心跳Job的方式保证，raft同步数据只需要一轮请求，而ZAB需要两轮，不过raft在宕机时也更容易丢失数据

## [一致性hash](https://github.com/syllr/leanring_notes/blob/master/Typora/Redis.md#%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95)

## Quorum NWR

基于多数派读的概念，扩展一下其实不仅当读和写是多数派的时候才能保证一致性，只要W+R>N，这样就能保证一致性，Quorum NWR算法将N，W，R三个参数动态化

举个例子，我们可以将集群中的数据通过tag分类，不同的tag的数据采用不同的NWR策略

* 对于大量写入但是极少读取的数据，例如报表数据，W可以设置的很小，R可以设置的比较大（要保证强数据一致性的话，必须满足W+R>N）
* 对于写入比较少，但是读取频繁的数据，W可以设置的比较大，R可以设置的比较小（要保证强数据一致性的话，必须满足W+R>N）
* 如果不需要满足强一致性，可以把W和R都设置的比较小（W+R<N），但是要保证一致性的话必须要进行一致性修复（读时修复，写时修复，反墒），这样的策略其实和Gossip差不多

NWR和其它的协议一样，只是一种思路，具体实现可以是多种多样的

* 可以利用给数据打tag来给数据分类进而使用不同的读写策略
* 可以给数据加上版本号或者时间戳来判断是否是最新的数据
* 考虑到数据的高可用性，W应该大于1，因为如果W=1，当这个节点宕机，并且数据无法恢复，那么这个数据就丢失了
* 要考虑到集群节点的动态扩张或者收缩，可以吧W，R，N之间的关系设置成比例

## PBFT

兰伯特在论文中提到有两种解决拜占庭容错的方法：口头协议和书面协议，但是这两个方法都其实都是递归的思想，随着集群规模的增长，消息数量指数级暴增

PBFT 全称 Practical Byzantine-Fault-Tolerant中文名为实用拜占庭容错算法，PBFT和书面协议一样都引入了消息签名在保证作恶节点无法篡改消息，于书面协议不同的是，PBFT将达成共识的过程分解成了三个阶段。

### 书面协议无法落地

因为书面协议引入了签名机制，可以防止Assistant是叛徒的时候修改Commander的命令，但是这个前提是Commander不是叛徒，如果Commander和某个Assistant都是叛徒，Assistant也可以篡改Commander的消息，其实书面协议大致分为两种情况

* Commander不是叛徒

  > 因为有签名的机制，Assistant中的叛徒无法修改Commander的命令，叛徒最多只能不转发消息，相当于一个宕机节点，这个时候其实只需要考虑信息不可达的情况，这种场景其实和Raft以及ZAB协议一样

* Commander是叛徒

  > 签名机制无法预防Commander作为叛徒发送不同的消息给Assistant，同样需要向别的Assistant确认Commander的信息，这是一个递归的过程，也是造成复杂度爆炸的原因

### PBFT对于书面协议的改进

PBFT先对Commander进行验证

* Commander不是叛徒：执行同步逻辑
* Commander是叛徒：通过“轮流上岗”的方式最终选出一个忠诚、稳定运行新主节点，并保证了共识的达成

PBFT通过两种机制来实现对Comannder身份的判断

* 三阶段共识
* 视图变更

### 三阶段共识

首先，客户端向主节点发起请求，主节点收到客户端请求，会向其它节点发送 pre-prepare 消息，其它节点就收到了pre-prepare 消息，就开始了这个核心三阶段共识过程了

1. Pre-prepare 阶段：主节点收到客户端请求之后，向其它节点发送pre-prepare消息

   * 节点收到 pre-prepare 消息后，会有两种选择，一种是接受，一种是不接受。什么时候才不接受主节点发来的 pre-prepare 消息呢？
   * 一种典型的情况就是如果一个节点接受到了一条 pre-prepare 消息，消息里的 v 和 n 在之前收到里的消息是曾经出现过的，但是 d 和 m 却和之前的消息不一致，或者请求编号不在高低水位之间，这时候就会拒绝请求。拒绝的逻辑就是主节点不会发送两条具有相同的 v 和 n ，但 d 和 m 却不同的消息

   > Pre-prepare消息里面的包含的数据
   >
   > V：当前视图的编号。视图的编号是什么意思呢？比如当前主节点为 A，视图编号为 1，如果主节点换成 B，那么视图编号就为 2，这个概念和 raft 的 term 任期是很类似的。
   > N：当前请求的编号。主节点收到客户端的每个请求都以一个编号来标记。
   > M：消息的内容
   > d或D（m）：消息内容的摘要（验签）
   > i：节点的编号

2. Prepare 阶段：节点同意请求后会向其它节点发送 prepare 消息。这里要注意一点，同一时刻不是只有一个节点在进行这个过程，可能有 n 个节点也在进行这个过程。因此节点是有可能收到其它节点发送的 prepare 消息的。在一定时间范围内（应对故障行为），如果收到超过 2f 个不同节点的 prepare 消息，就代表 prepare 阶段已经完成

   > Prepare消息基本上就是Pre-prepare消息的转发，但是会加上当前转发的节点，和http请求头中Referer含义差不多，就是起一个转发的作用，集群中的每个节点都能拿到主节点对所有节点的数据视图，节点可以根据这个判断出主节点是不是叛徒

3. Commit 阶段：于是进入 commit 阶段。向其它节点广播 commit 消息，同理，这个过程可能是有 n 个节点也在进行的。因此可能会收到其它节点发过来的 commit 消息，当收到 2f+1 个 commit 消息后（包括自己），代表大多数节点已经进入 commit 阶段，这一阶段已经达成共识，于是节点就会执行请求，写入数据

   > 当节点判断当前主节点不是叛徒之后，进行提交，超过一定数量的节点都判断出主节点不是叛徒，那么主节点的数据就会被达成共识

### 视图变更

> 当节点判断当前主节点不是叛徒之后，进行提交，超过一定数量的节点都判断出主节点不是叛徒，那么主节点的数据就会被达成共识

如果没有超过一定数量的节点判断出当前主节点不是叛徒，那么主节点就会被替换，直到选出一个不是叛徒的节点
